<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>分布式缓存一致性问题 | HyperTars' Blog</title><meta name=keywords content="Distributed System,Distributed Cache Consistency,Redis"><meta name=description content="Introduction to distributed cache consistency"><meta name=author content="HyperTars"><link rel=canonical href=https://blog.hypertars.com/posts/developer/distributed_systems/cache/><link crossorigin=anonymous href=/assets/css/stylesheet.min.f34b707e96a5011787260c43994ea6cd89c86f5fef677a5e3c78f655715fd662.css integrity="sha256-80twfpalAReHJgxDmU6mzYnIb1/vZ3pePHj2VXFf1mI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://blog.hypertars.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.hypertars.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.hypertars.com/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.hypertars.com/apple-touch-icon.png><link rel=mask-icon href=https://blog.hypertars.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="分布式缓存一致性问题"><meta property="og:description" content="Introduction to distributed cache consistency"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.hypertars.com/posts/developer/distributed_systems/cache/"><meta property="og:image" content="https://blog.hypertars.com/posts/developer/distributed_systems/cache/https:/hazelcast.com/wp-content/uploads/2021/12/39_Distributed-Cache.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-04T00:00:00+08:00"><meta property="article:modified_time" content="2021-05-04T00:00:00+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.hypertars.com/posts/developer/distributed_systems/cache/https:/hazelcast.com/wp-content/uploads/2021/12/39_Distributed-Cache.png"><meta name=twitter:title content="分布式缓存一致性问题"><meta name=twitter:description content="Introduction to distributed cache consistency"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Articles","item":"https://blog.hypertars.com/posts/"},{"@type":"ListItem","position":3,"name":"Developer","item":"https://blog.hypertars.com/posts/developer/"},{"@type":"ListItem","position":4,"name":"Distributed System","item":"https://blog.hypertars.com/posts/developer/distributed_systems/"},{"@type":"ListItem","position":5,"name":"分布式缓存一致性问题","item":"https://blog.hypertars.com/posts/developer/distributed_systems/cache/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"分布式缓存一致性问题","name":"分布式缓存一致性问题","description":"Introduction to distributed cache consistency","keywords":["Distributed System","Distributed Cache Consistency","Redis"],"articleBody":"分布式缓存一致性 缓存系统属于 CAP 中的 AP，BASE 理论，没法强一致性，只能最终一致性\n缓存策略 Cache Aside  对于读操作，先读缓存，读不到就读db，然后回写缓存 对于写操作，会先更新db，然后删除缓存（使缓存失效）。 为什么是删除缓存而不是更新缓存呢？  Read-Through Read Through: 查询操作中更新缓存 存储自己维护自己的 Cache\nRead-Through 和 Cache Aside 很像，但是有两个不同点：\n Cache Aside 在缓存 miss 的时候读 db 回写 cache 是在业务代码（Application）中支持该逻辑的，而 Read-Through 是由其他系统回写 cache 的 Cache Aside 由于回写 cache 是业务代码自身控制，所以自由度很高，支持 db 和缓存的数据模型不一致的情况（甚至是跨表跨数据库）；而 Read-Through 只能支持相同数据模型（其实也未必，但是不能跨表和跨数据库是确定的。而且多行对应一个缓存key也不好处理）  Write-Through 更新缓存，然后由 Cache 自己更新数据库 存储自己维护自己的 Cache\n虽然看上去 Write-Through 貌似没啥用，但是配合 Read-Through，可以保证数据的最终一致性，而且不需要在更新 DB 后删除缓存。具体参考 AWS DAX\nWrite Back (Write Behind Caching) 先更新缓存，写入缓存成功后直接返回，经过一些延迟后，将缓存中的数据回写db（批量异步更新数据库）\n 非强一致性，可能丢失 Lazy Write  这种方式类似于 Page Cache 算法（会丢）、mysql 的 change buffer（不会丢）等常规的算法。\n 这种方式能极高地提升写入性能，适用于写入 QPS 极高的服务，配合 Read-Through 可以让数据有很好的一致性。 可以容忍 DB 故障一段时间。 如果 Cache 故障，可能会丢数据  缓存实践：不引入外部系统，纯业务代码处理 Case1 写 DB 成功后更新缓存（不推荐） 对于读请求，先从缓存中读取，如果读取不到，再从 DB 读取，然后写入缓存。 对于写请求，先更新 DB，成功后，更新缓存\n存在问题\n 如果同一个操作时间内，有两个以上的更新操作，存在并发更新导致旧更新缓存覆盖了新更新缓存的情况。（如果操作满足 CRDT，那么即使时序不一致，数据也能保证最终一致性）  如果写 DB 成功，更新缓存失败，会导致数据不一致。  Case2 写 DB 成功后删除缓存 对于写请求，先更新 DB，成功后，删除缓存\n存在问题\n 对于写请求，更新 DB 成功，删除缓存失败，会导致数据不一致 一个读操作，没有命中缓存，然后到 DB 取数据，此时来了一个写操作，写完 DB 之后，删除缓存，然后读操作再把老的数据读出来放进缓存（几率非常低，RR 隔离级别） 当写操作很频繁的时候，缓存会一直被删除，导致请求一直打到 DB，有打爆 DB 的风险，而且还有主从延迟的问题。  Facebook 有关于该方式的详细论文介绍，里面详细描述了在生产环境中 facebook 是如何使用该方式保证缓存和 DB 的一致性的， Scaling Memcache at Facebook\nCase3 删除缓存后再更新 DB （不推荐） 对于 case2，有同学可能会想到，那我先删除缓存，再修改 DB可以吗？下面我们讨论一下这种case。\n 对于写请求，先删除缓存，再更新 DB  存在问题\n 如果在删除缓存后，更新 DB 前，来了一个新的读请求，读到了还没更新的 DB 数据，更新到了缓存，缓存和 DB 会不一致 当写操作很频繁的时候，缓存会一直被删除，导致请求一直打到 DB，有打爆 DB 的风险  小结  没有引入任何额外的组件，系统复杂度低；直接操作存储，耗时点，更新快 由于 Cache Aside 无法保证操作同时切成功作用在 DB 和缓存中（即缓存和 DB 的整体事务问题），所以没办法百分百保证数据的一致性。对于这点，我们可以考虑通过 2PC、Paxos 等共识算法，或者另辟蹊径，使用补偿的方式，例如消费 binlog，通过 MQ 进行补偿重试来解决。  有的同学可能会想到：开一个事务，执行写 DB 和更新/删除缓存的操作，如果写缓存失败，就回滚事务。但是 mysql 的连接很宝贵，并发安全写缓存的操作可能会耗尽 mysql 连接。 在并发场景下，没办法保证更新操作的有序性。关于这点，我们可以利用系统本身的有序性来解决，例如利用 binlog 来保证有序性，更新或删除缓存。    缓存实践：引入外部系统结合解决问题 Case1 消费 binlog 删除缓存 写数据\n读数据\n优势\n 写操作不再需要耦合缓存操作，与业务代码解耦 解决了删缓存失败导致数据不一致的问题 能保证数据的最终一致性  存在问题\n 耗时长，缓存和 DB 存在一定的延迟  因为消费 binlog 到删除缓存这个步骤平均延迟大概 1-3 s，所以会有短时间存在缓存和 DB 不一致的问题。   当更新很频繁的时候，缓存会一直被删除，导致请求一直打到db，有打爆db的风险  Case2 双删 根据Case1 的问题，我们在业务代码更新 DB 成功后也删除一次缓存，来保证实时性。使用 binlog 的删除来保证数据的最终一致性。\n适用场景 该方式比较适合没有热点数据，写请求qps 不高或者大部分都是读几乎没有写的服务\nCase3 消费 binlog 更新缓存 优势\n 解耦了业务代码和缓存操作。 因为 binlog 是有序的，所以数据的最终一致性得到了保证  存在问题\n binlog 延迟比较大，一般 200ms 缓存才能更新 对于一些更新了的数据，不一定会在最近读到，饿汉模式可能会耗费更多的缓存资源 (饿汉模式：缓存本身是解决热点数据问题，所以并不是所有数据都要放入缓存中) 对于 DB 和缓存映射规则复杂（例如多对一）的场景不适用  Case4 双更新 由于通过消费 binlog 更新缓存延迟比较大，我们参考 Case2 的做法，更新 DB 后再更新缓存，然后通过消费binlog 的方式保证数据的最终一致性。\n优势 解决了Case3 更新后 DB 和 缓存短时间内不一致的问题\n存在问题 相比 Case3 ，同时也带来了双写 DB 和缓存时并发写导致短时间的缓存脏数据问题\nCase5 Write Back 优势\n 写性能发挥到了极致，全都走高速缓存 不会存在脏数据的问题（因为读写都是同一个数据源，顶多只会存在主从延迟的问题）  存在问题\n 数据有丢的可能（crash-safe 完全依赖于缓存本身） 目前很多 NoSQL 数据库对事务支持得不太好，对于需要保证事务的业务需要选择对事务支持比较完善的缓存（下一代分布式事务KV ByteKV详细设计 ） Cache 回写 DB 的过程需要额外设计。例如 cache 容量不够了，需要淘汰数据时，需要先落到 DB；哪些数据适合先回写 DB；什么时候回写 DB 等等。  Case6 Read/Write Through 优势 对业务很友好。业务不需要在业务代码中处理关于缓存和db的关系，通过sdk 直接操作即可。\n存在问题 只能保证最终一致性，对于数据敏感的业务不适用。（取决于Read/Write Through 实现）\n综合选型    缓存策略 优势 存在问题 适用场景 实践     写 DB 成功后删除缓存 1. db和缓存一致率比较高\n2. 没引入外部系统，复杂度低 1. 删缓存失败会导致数据不一\n2. 对于更新很频繁的业务，删缓存的方式可能会导致缓存穿透 1. 读多写少\n2. 可以接受db和缓存有（过期时间内）不一致 Scaling Memcache in Facebook   消费binlog删除缓存 1. 写操作不再需要耦合缓存操作，与业务代码解耦\n2. 解决了写缓存失败导致数据不一致的问题\n3. 能保证数据的最终一致性 1. 耗时长，缓存和 DB 存在一定的延迟\n2. 对于更新很频繁的业务，删缓存的方式可能会导致缓存穿透 1. 读多写少\n2. 可以接受db和缓存有短暂时间的不一致，不存在写后马上读的场景，但是需要保证最终一致性。    双删 消费binlog删除缓存 的全部优点，但是解决了延迟问题 1. 对于更新很频繁的业务，删缓存的方式可能会导致缓存穿透\n2. 在主从延迟的情况，读请求可能还是会读到脏数据 1. 读多写少\n2. 需要保证数据的最终一致性    消费binlog更新缓存 1. 解耦了业务代码和缓存操作。\n2. 因为binlog 是有序的，所以数据的最终一致性得到了保证\n3. 不存在删缓存导致穿透的问题 1. binlog 延迟比较大，一般1s左右 缓存才能更新\n2. 对于一些更新了的数据，不一定会在最近读到，饿汉模式可能会耗费更多的缓存资源\n3. 对于db 和 缓存映射规则复杂（例如多对一）的场景不适用 1. 可以接受db和缓存有短暂时间的不一致，但是需要保证最终一致性\n2. 缓存资源充足\n3. db和缓存映射规则简单\n4. 要求缓存命中率高\n5. 修改cache 的成本不高    双更新 消费binlog更新缓存  的全部优点，但是解决了更新后 DB 和缓存短时间内不一致的问题 同时也带来了双写 DB 和缓存时并发写导致短时间的缓存脏数据问题 1. 可以接受缓存数据有小概率短时间不正确，但是需要保证最终一致性\n2. 缓存资源充足\n3. DB 和缓存映射规则简单\n4. 要求缓存命中率高\n5. 修改cache 的成本不高    Write Back 1. 写性能发挥到了极致，全都走高速缓存\n2. 不会存在脏数据的问题（因为读写都是同一个数据源，顶多只会存在主从延迟的问题） 1. 数据有丢的可能（crash-safe 完全依赖于缓存本身）\n2. 目前很多NoSQL数据库对事务支持得不太好，对于需要保证事务的业务需要选择对事务支持比较完善的缓存\n3. Cache 回写db的过程需要额外设计。例如cache 容量不够 1. 写 QPS 非常高，DB 没办法抗住。（或者要求极高的写性能）\n2. 缓存支持 crash-safe ，故障之后不会丢数据或者业务允许丢数据\n3. 对事务要求不高\n4. 有成熟的cache 回写db 策略。    Read/Write Through 对业务很友好。业务不需要在业务代码中处理关于缓存和 DB 的关系，通过 SDK 直接操作即可。 只能保证最终一致性，对于数据敏感的业务不适用。（取决于Read/Write Through 实现） 1. 读多写少\n2. 不要求强一致读\n3. 业务发展迅速，需要快速迭代开发 AWS DAX Facebook TAO    ","wordCount":"3481","inLanguage":"en","image":"https://blog.hypertars.com/posts/developer/distributed_systems/cache/https:/hazelcast.com/wp-content/uploads/2021/12/39_Distributed-Cache.png","datePublished":"2021-05-04T00:00:00+08:00","dateModified":"2021-05-04T00:00:00+08:00","author":[{"@type":"Person","name":"HyperTars"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.hypertars.com/posts/developer/distributed_systems/cache/"},"publisher":{"@type":"Organization","name":"HyperTars' Blog","logo":{"@type":"ImageObject","url":"https://blog.hypertars.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.hypertars.com accesskey=h title="HyperTars' Blog (Alt + H)">HyperTars' Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://hypertars.com title=🏠Home><span>🏠Home</span></a></li><li><a href=https://blog.hypertars.com/search/ title="🔍Search (Alt + /)" accesskey=/><span>🔍Search</span></a></li><li><a href=https://blog.hypertars.com/posts/developer/ title=👨🏻‍💻Developer><span>👨🏻‍💻Developer</span></a></li><li><a href=https://blog.hypertars.com/archives/ title=⏱Archives><span>⏱Archives</span></a></li><li><a href=https://blog.hypertars.com/tags/ title=🔖tags><span>🔖tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.hypertars.com>Home</a>&nbsp;»&nbsp;<a href=https://blog.hypertars.com/posts/>Articles</a>&nbsp;»&nbsp;<a href=https://blog.hypertars.com/posts/developer/>Developer</a>&nbsp;»&nbsp;<a href=https://blog.hypertars.com/posts/developer/distributed_systems/>Distributed System</a></div><h1 class=post-title>分布式缓存一致性问题</h1><div class=post-description>Introduction to distributed cache consistency</div><div class=post-meta><span title="2021-05-04 00:00:00 +0800 +0800">2021-05-04</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;HyperTars</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%88%86%e5%b8%83%e5%bc%8f%e7%bc%93%e5%ad%98%e4%b8%80%e8%87%b4%e6%80%a7 aria-label=分布式缓存一致性>分布式缓存一致性</a><ul><li><a href=#%e7%bc%93%e5%ad%98%e7%ad%96%e7%95%a5 aria-label=缓存策略>缓存策略</a><ul><li><a href=#cache-aside aria-label="Cache Aside">Cache Aside</a></li><li><a href=#read-through aria-label=Read-Through>Read-Through</a></li><li><a href=#write-through aria-label=Write-Through>Write-Through</a></li><li><a href=#write-back-write-behind-caching aria-label="Write Back (Write Behind Caching)">Write Back (Write Behind Caching)</a></li></ul></li><li><a href=#%e7%bc%93%e5%ad%98%e5%ae%9e%e8%b7%b5%e4%b8%8d%e5%bc%95%e5%85%a5%e5%a4%96%e9%83%a8%e7%b3%bb%e7%bb%9f%e7%ba%af%e4%b8%9a%e5%8a%a1%e4%bb%a3%e7%a0%81%e5%a4%84%e7%90%86 aria-label=缓存实践：不引入外部系统，纯业务代码处理>缓存实践：不引入外部系统，纯业务代码处理</a><ul><li><a href=#case1-%e5%86%99-db-%e6%88%90%e5%8a%9f%e5%90%8e%e6%9b%b4%e6%96%b0%e7%bc%93%e5%ad%98%e4%b8%8d%e6%8e%a8%e8%8d%90 aria-label="Case1 写 DB 成功后更新缓存（不推荐）">Case1 写 DB 成功后更新缓存（不推荐）</a></li><li><a href=#case2-%e5%86%99-db-%e6%88%90%e5%8a%9f%e5%90%8e%e5%88%a0%e9%99%a4%e7%bc%93%e5%ad%98 aria-label="Case2 写 DB 成功后删除缓存">Case2 写 DB 成功后删除缓存</a></li><li><a href=#case3-%e5%88%a0%e9%99%a4%e7%bc%93%e5%ad%98%e5%90%8e%e5%86%8d%e6%9b%b4%e6%96%b0-db-%e4%b8%8d%e6%8e%a8%e8%8d%90 aria-label="Case3 删除缓存后再更新 DB （不推荐）">Case3 删除缓存后再更新 DB （不推荐）</a></li><li><a href=#%e5%b0%8f%e7%bb%93 aria-label=小结>小结</a></li></ul></li><li><a href=#%e7%bc%93%e5%ad%98%e5%ae%9e%e8%b7%b5%e5%bc%95%e5%85%a5%e5%a4%96%e9%83%a8%e7%b3%bb%e7%bb%9f%e7%bb%93%e5%90%88%e8%a7%a3%e5%86%b3%e9%97%ae%e9%a2%98 aria-label=缓存实践：引入外部系统结合解决问题>缓存实践：引入外部系统结合解决问题</a><ul><li><a href=#case1-%e6%b6%88%e8%b4%b9-binlog-%e5%88%a0%e9%99%a4%e7%bc%93%e5%ad%98 aria-label="Case1 消费 binlog 删除缓存">Case1 消费 binlog 删除缓存</a></li><li><a href=#case2-%e5%8f%8c%e5%88%a0 aria-label="Case2 双删">Case2 双删</a></li><li><a href=#case3-%e6%b6%88%e8%b4%b9-binlog-%e6%9b%b4%e6%96%b0%e7%bc%93%e5%ad%98 aria-label="Case3 消费 binlog 更新缓存">Case3 消费 binlog 更新缓存</a></li><li><a href=#case4-%e5%8f%8c%e6%9b%b4%e6%96%b0 aria-label="Case4 双更新">Case4 双更新</a></li><li><a href=#case5-write-back aria-label="Case5 Write Back">Case5 Write Back</a></li><li><a href=#case6-readwrite-through aria-label="Case6 Read/Write Through">Case6 Read/Write Through</a></li></ul></li><li><a href=#%e7%bb%bc%e5%90%88%e9%80%89%e5%9e%8b aria-label=综合选型>综合选型</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const e=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${e}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=分布式缓存一致性>分布式缓存一致性<a hidden class=anchor aria-hidden=true href=#分布式缓存一致性>#</a></h2><p>缓存系统属于 CAP 中的 AP，BASE 理论，没法强一致性，只能最终一致性</p><h3 id=缓存策略>缓存策略<a hidden class=anchor aria-hidden=true href=#缓存策略>#</a></h3><h4 id=cache-aside>Cache Aside<a hidden class=anchor aria-hidden=true href=#cache-aside>#</a></h4><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/dd0da41dc3d620c473aed40ade4095f.png width=500 align=center>
<img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/fddfcf61ac44a8ea0d771c45c0f5214.png width=500 align=center><ul><li>对于读操作，先读缓存，读不到就读db，然后回写缓存</li><li>对于写操作，会<strong>先更新db，然后删除缓存</strong>（使缓存失效）。</li><li><a href=https://www.quora.com/Why-does-Facebook-use-delete-to-remove-the-key-value-pair-in-Memcached-instead-of-updating-the-Memcached-during-write-request-to-the-backend>为什么是删除缓存而不是更新缓存呢？</a></li></ul><h4 id=read-through>Read-Through<a hidden class=anchor aria-hidden=true href=#read-through>#</a></h4><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/8793305ac06d808620de3e0f2d24d3d.png width=500 align=center><p>Read Through: 查询操作中更新缓存
存储自己维护自己的 Cache</p><p>Read-Through 和 Cache Aside 很像，但是有两个不同点：</p><ol><li>Cache Aside 在缓存 miss 的时候读 db 回写 cache 是在业务代码（Application）中支持该逻辑的，而 Read-Through 是由<strong>其他系统回写 cache</strong> 的</li><li>Cache Aside 由于回写 cache 是业务代码自身控制，所以自由度很高，支持 db 和缓存的数据模型不一致的情况（甚至是跨表跨数据库）；而 Read-Through 只能支持相同数据模型（其实也未必，但是不能跨表和跨数据库是确定的。而且多行对应一个缓存key也不好处理）</li></ol><h4 id=write-through>Write-Through<a hidden class=anchor aria-hidden=true href=#write-through>#</a></h4><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/76829f94b5472016c57ae923e16e42b.png width=500 align=center><p>更新缓存，然后由 Cache 自己更新数据库
存储自己维护自己的 Cache</p><p>虽然看上去 Write-Through 貌似没啥用，但是配合 Read-Through，可以保证数据的最终一致性，而且不需要在更新 DB 后删除缓存。具体参考 <a href=https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.consistency.html>AWS DAX</a></p><h4 id=write-back-write-behind-caching>Write Back (Write Behind Caching)<a hidden class=anchor aria-hidden=true href=#write-back-write-behind-caching>#</a></h4><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/55fa962af0bb5571fe905c179810299.png width=500 align=center><p>先更新缓存，写入缓存成功后直接返回，经过一些延迟后，将缓存中的数据回写db（批量异步更新数据库）</p><ul><li>非强一致性，可能丢失</li><li>Lazy Write</li></ul><p>这种方式类似于 Page Cache 算法（会丢）、mysql 的 change buffer（不会丢）等常规的算法。</p><ol><li>这种方式能极高地提升写入性能，适用于写入 QPS 极高的服务，配合 Read-Through 可以让数据有很好的一致性。</li><li>可以容忍 DB 故障一段时间。</li><li>如果 Cache 故障，可能会丢数据</li></ol><h3 id=缓存实践不引入外部系统纯业务代码处理>缓存实践：不引入外部系统，纯业务代码处理<a hidden class=anchor aria-hidden=true href=#缓存实践不引入外部系统纯业务代码处理>#</a></h3><h4 id=case1-写-db-成功后更新缓存不推荐>Case1 写 DB 成功后更新缓存（不推荐）<a hidden class=anchor aria-hidden=true href=#case1-写-db-成功后更新缓存不推荐>#</a></h4><p>对于读请求，先从缓存中读取，如果读取不到，再从 DB 读取，然后写入缓存。
对于写请求，先更新 DB，成功后，更新缓存</p><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/e93b0b38b535220821e2f278713f655.png width=200 align=center><p><strong>存在问题</strong></p><ol><li>如果同一个操作时间内，有两个以上的更新操作，存在并发更新导致旧更新缓存覆盖了新更新缓存的情况。（如果操作满足 <a href=https://hal.inria.fr/file/index/docid/555588/filename/techreport.pdf>CRDT</a>，那么即使时序不一致，数据也能保证最终一致性）</li></ol><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/7deb9eccb549e1f28c1a592e252eb0a.jpg width=200 align=center><ol start=2><li>如果写 DB 成功，更新缓存失败，会导致数据不一致。</li></ol><h4 id=case2-写-db-成功后删除缓存>Case2 写 DB 成功后删除缓存<a hidden class=anchor aria-hidden=true href=#case2-写-db-成功后删除缓存>#</a></h4><p>对于写请求，先更新 DB，成功后，删除缓存</p><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/207eb548951a3ac6dccfb808c082f9e.png width=400 align=center><p><strong>存在问题</strong></p><ol><li>对于写请求，更新 DB 成功，删除缓存失败，会导致数据不一致</li><li>一个读操作，没有命中缓存，然后到 DB 取数据，此时来了一个写操作，写完 DB 之后，删除缓存，然后读操作再把老的数据读出来放进缓存（几率非常低，RR 隔离级别）</li><li>当写操作很频繁的时候，缓存会一直被删除，导致请求一直打到 DB，有打爆 DB 的风险，而且还有主从延迟的问题。</li></ol><p>Facebook 有关于该方式的详细论文介绍，里面详细描述了在生产环境中 facebook 是如何使用该方式保证缓存和 DB 的一致性的， <a href=https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf>Scaling Memcache at Facebook</a></p><h4 id=case3-删除缓存后再更新-db-不推荐>Case3 删除缓存后再更新 DB （不推荐）<a hidden class=anchor aria-hidden=true href=#case3-删除缓存后再更新-db-不推荐>#</a></h4><p>对于 case2，有同学可能会想到，那我先删除缓存，再修改 DB可以吗？下面我们讨论一下这种case。</p><ul><li>对于写请求，先删除缓存，再更新 DB</li></ul><p><strong>存在问题</strong></p><ol><li>如果在删除缓存后，更新 DB 前，来了一个新的读请求，读到了还没更新的 DB 数据，更新到了缓存，缓存和 DB 会不一致</li><li>当写操作很频繁的时候，缓存会一直被删除，导致请求一直打到 DB，有打爆 DB 的风险</li></ol><h4 id=小结>小结<a hidden class=anchor aria-hidden=true href=#小结>#</a></h4><ol><li>没有引入任何额外的组件，系统复杂度低；直接操作存储，耗时点，更新快</li><li>由于 Cache Aside 无法保证操作同时切成功作用在 DB 和缓存中（即缓存和 DB 的整体事务问题），所以没办法百分百保证数据的一致性。对于这点，我们可以考虑通过 2PC、Paxos 等共识算法，或者另辟蹊径，使用补偿的方式，例如消费 binlog，通过 MQ 进行补偿重试来解决。<ul><li>有的同学可能会想到：开一个事务，执行写 DB 和更新/删除缓存的操作，如果写缓存失败，就回滚事务。但是 mysql 的连接很宝贵，并发安全写缓存的操作可能会耗尽 mysql 连接。</li><li>在并发场景下，没办法保证更新操作的有序性。关于这点，我们可以利用系统本身的有序性来解决，例如利用 binlog 来保证有序性，更新或删除缓存。</li></ul></li></ol><h3 id=缓存实践引入外部系统结合解决问题>缓存实践：引入外部系统结合解决问题<a hidden class=anchor aria-hidden=true href=#缓存实践引入外部系统结合解决问题>#</a></h3><h4 id=case1-消费-binlog-删除缓存>Case1 消费 binlog 删除缓存<a hidden class=anchor aria-hidden=true href=#case1-消费-binlog-删除缓存>#</a></h4><p>写数据</p><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/9956dd3aa42275dd0a3510b6716cde0.jpg width=500 align=center><p>读数据</p><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/a9b8c549d0bb0bfee5248d3a2bd2e76.jpg width=300 align=center><p><strong>优势</strong></p><ol><li>写操作不再需要耦合缓存操作，与业务代码解耦</li><li>解决了删缓存失败导致数据不一致的问题</li><li>能保证数据的最终一致性</li></ol><p><strong>存在问题</strong></p><ol><li>耗时长，缓存和 DB 存在一定的延迟<ul><li>因为消费 binlog 到删除缓存这个步骤平均延迟大概 1-3 s，所以会有短时间存在缓存和 DB 不一致的问题。</li></ul></li><li>当更新很频繁的时候，缓存会一直被删除，导致请求一直打到db，有打爆db的风险</li></ol><h4 id=case2-双删>Case2 双删<a hidden class=anchor aria-hidden=true href=#case2-双删>#</a></h4><p>根据Case1 的问题，我们在业务代码更新 DB 成功后也删除一次缓存，来保证实时性。使用 binlog 的删除来保证数据的最终一致性。</p><p><strong>适用场景</strong>
该方式比较适合没有热点数据，写请求qps 不高或者大部分都是读几乎没有写的服务</p><h4 id=case3-消费-binlog-更新缓存>Case3 消费 binlog 更新缓存<a hidden class=anchor aria-hidden=true href=#case3-消费-binlog-更新缓存>#</a></h4><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/f48ddfd995e71fc1d81f76be21fd755.png width=300 align=center><p><strong>优势</strong></p><ol><li>解耦了业务代码和缓存操作。</li><li>因为 binlog 是有序的，所以数据的最终一致性得到了保证</li></ol><p><strong>存在问题</strong></p><ol><li>binlog 延迟比较大，一般 200ms 缓存才能更新</li><li>对于一些更新了的数据，不一定会在最近读到，饿汉模式可能会耗费更多的缓存资源 (饿汉模式：缓存本身是解决热点数据问题，所以并不是所有数据都要放入缓存中)</li><li>对于 DB 和缓存映射规则复杂（例如多对一）的场景不适用</li></ol><h4 id=case4-双更新>Case4 双更新<a hidden class=anchor aria-hidden=true href=#case4-双更新>#</a></h4><p>由于通过消费 binlog 更新缓存延迟比较大，我们参考 Case2 的做法，更新 DB 后再更新缓存，然后通过消费binlog 的方式保证数据的最终一致性。</p><p><strong>优势</strong>
解决了Case3 更新后 DB 和 缓存短时间内不一致的问题</p><p><strong>存在问题</strong>
相比 Case3 ，同时也带来了双写 DB 和缓存时并发写导致短时间的缓存脏数据问题</p><h4 id=case5-write-back>Case5 Write Back<a hidden class=anchor aria-hidden=true href=#case5-write-back>#</a></h4><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/55fa962af0bb5571fe905c179810299.png width=500 align=center><p><strong>优势</strong></p><ol><li>写性能发挥到了极致，全都走高速缓存</li><li>不会存在脏数据的问题（因为读写都是同一个数据源，顶多只会存在主从延迟的问题）</li></ol><p><strong>存在问题</strong></p><ol><li>数据有丢的可能（crash-safe 完全依赖于缓存本身）</li><li>目前很多 NoSQL 数据库对事务支持得不太好，对于需要保证事务的业务需要选择对事务支持比较完善的缓存（下一代分布式事务KV ByteKV详细设计 ）</li><li>Cache 回写 DB 的过程需要额外设计。例如 cache 容量不够了，需要淘汰数据时，需要先落到 DB；哪些数据适合先回写 DB；什么时候回写 DB 等等。</li></ol><h4 id=case6-readwrite-through>Case6 Read/Write Through<a hidden class=anchor aria-hidden=true href=#case6-readwrite-through>#</a></h4><img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/8793305ac06d808620de3e0f2d24d3d.png width=500 align=center>
<img src=https://s3.us-west-1.amazonaws.com/images.hypertars.com/cache/76829f94b5472016c57ae923e16e42b.png width=500 align=center><p><strong>优势</strong>
对业务很友好。业务不需要在业务代码中处理关于缓存和db的关系，通过sdk 直接操作即可。</p><p><strong>存在问题</strong>
只能保证最终一致性，对于数据敏感的业务不适用。（取决于Read/Write Through 实现）</p><h3 id=综合选型>综合选型<a hidden class=anchor aria-hidden=true href=#综合选型>#</a></h3><table><thead><tr><th>缓存策略</th><th>优势</th><th>存在问题</th><th>适用场景</th><th>实践</th></tr></thead><tbody><tr><td><code>写 DB 成功后删除缓存</code></td><td>1. db和缓存一致率比较高<br>2. 没引入外部系统，复杂度低</td><td>1. 删缓存失败会导致数据不一<br>2. 对于更新很频繁的业务，删缓存的方式可能会导致缓存穿透</td><td>1. 读多写少<br>2. 可以接受db和缓存有（过期时间内）不一致</td><td><a href=https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf>Scaling Memcache in Facebook</a></td></tr><tr><td><code>消费binlog删除缓存</code></td><td>1. 写操作不再需要耦合缓存操作，与业务代码解耦<br>2. 解决了写缓存失败导致数据不一致的问题<br>3. 能保证数据的最终一致性</td><td>1. 耗时长，缓存和 DB 存在一定的延迟<br>2. 对于更新很频繁的业务，删缓存的方式可能会导致缓存穿透</td><td>1. 读多写少<br>2. 可以接受db和缓存有短暂时间的不一致，不存在写后马上读的场景，但是需要保证最终一致性。</td><td></td></tr><tr><td><code>双删</code></td><td><code>消费binlog删除缓存</code> 的全部优点，但是解决了延迟问题</td><td>1. 对于更新很频繁的业务，删缓存的方式可能会导致缓存穿透<br>2. 在主从延迟的情况，读请求可能还是会读到脏数据</td><td>1. 读多写少<br>2. 需要保证数据的最终一致性</td><td></td></tr><tr><td><code>消费binlog更新缓存</code></td><td>1. 解耦了业务代码和缓存操作。<br>2. 因为binlog 是有序的，所以数据的最终一致性得到了保证<br>3. 不存在删缓存导致穿透的问题</td><td>1. binlog 延迟比较大，一般1s左右 缓存才能更新<br>2. 对于一些更新了的数据，不一定会在最近读到，饿汉模式可能会耗费更多的缓存资源<br>3. 对于db 和 缓存映射规则复杂（例如多对一）的场景不适用</td><td>1. 可以接受db和缓存有短暂时间的不一致，但是需要保证最终一致性<br>2. 缓存资源充足<br>3. db和缓存映射规则简单<br>4. 要求缓存命中率高<br>5. 修改cache 的成本不高</td><td></td></tr><tr><td><code>双更新</code></td><td><code>消费binlog更新缓存</code> 的全部优点，但是解决了更新后 DB 和缓存短时间内不一致的问题</td><td>同时也带来了双写 DB 和缓存时并发写导致短时间的缓存脏数据问题</td><td>1. 可以接受缓存数据有小概率短时间不正确，但是需要保证最终一致性<br>2. 缓存资源充足<br>3. DB 和缓存映射规则简单<br>4. 要求缓存命中率高<br>5. 修改cache 的成本不高</td><td></td></tr><tr><td><code>Write Back</code></td><td>1. 写性能发挥到了极致，全都走高速缓存<br>2. 不会存在脏数据的问题（因为读写都是同一个数据源，顶多只会存在主从延迟的问题）</td><td>1. 数据有丢的可能（crash-safe 完全依赖于缓存本身）<br>2. 目前很多NoSQL数据库对事务支持得不太好，对于需要保证事务的业务需要选择对事务支持比较完善的缓存<br>3. Cache 回写db的过程需要额外设计。例如cache 容量不够</td><td>1. 写 QPS 非常高，DB 没办法抗住。（或者要求极高的写性能）<br>2. 缓存支持 crash-safe ，故障之后不会丢数据或者业务允许丢数据<br>3. 对事务要求不高<br>4. 有成熟的cache 回写db 策略。</td><td></td></tr><tr><td><code>Read/Write Through</code></td><td>对业务很友好。业务不需要在业务代码中处理关于缓存和 DB 的关系，通过 SDK 直接操作即可。</td><td>只能保证最终一致性，对于数据敏感的业务不适用。（取决于Read/Write Through 实现）</td><td>1. 读多写少<br>2. 不要求强一致读<br>3. 业务发展迅速，需要快速迭代开发</td><td><a href=https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.consistency.html>AWS DAX</a> <a href=https://www.usenix.org/system/files/conference/atc13/atc13-bronson.pdf>Facebook TAO</a></td></tr></tbody></table></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.hypertars.com/tags/distributed-system/>Distributed System</a></li><li><a href=https://blog.hypertars.com/tags/distributed-cache-consistency/>Distributed Cache Consistency</a></li><li><a href=https://blog.hypertars.com/tags/redis/>Redis</a></li></ul><nav class=paginav><a class=prev href=https://blog.hypertars.com/posts/developer/database/cassandra/><span class=title>« Prev Page</span><br><span>Apache Cassandra</span></a>
<a class=next href=https://blog.hypertars.com/posts/developer/distributed_systems/raft/><span class=title>Next Page »</span><br><span>Raft 分布式共识算法</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 分布式缓存一致性问题 on twitter" href="https://twitter.com/intent/tweet/?text=%e5%88%86%e5%b8%83%e5%bc%8f%e7%bc%93%e5%ad%98%e4%b8%80%e8%87%b4%e6%80%a7%e9%97%ae%e9%a2%98&url=https%3a%2f%2fblog.hypertars.com%2fposts%2fdeveloper%2fdistributed_systems%2fcache%2f&hashtags=DistributedSystem%2cDistributedCacheConsistency%2cRedis"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 分布式缓存一致性问题 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fblog.hypertars.com%2fposts%2fdeveloper%2fdistributed_systems%2fcache%2f&title=%e5%88%86%e5%b8%83%e5%bc%8f%e7%bc%93%e5%ad%98%e4%b8%80%e8%87%b4%e6%80%a7%e9%97%ae%e9%a2%98&summary=%e5%88%86%e5%b8%83%e5%bc%8f%e7%bc%93%e5%ad%98%e4%b8%80%e8%87%b4%e6%80%a7%e9%97%ae%e9%a2%98&source=https%3a%2f%2fblog.hypertars.com%2fposts%2fdeveloper%2fdistributed_systems%2fcache%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 分布式缓存一致性问题 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblog.hypertars.com%2fposts%2fdeveloper%2fdistributed_systems%2fcache%2f&title=%e5%88%86%e5%b8%83%e5%bc%8f%e7%bc%93%e5%ad%98%e4%b8%80%e8%87%b4%e6%80%a7%e9%97%ae%e9%a2%98"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 分布式缓存一致性问题 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.hypertars.com%2fposts%2fdeveloper%2fdistributed_systems%2fcache%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 分布式缓存一致性问题 on whatsapp" href="https://api.whatsapp.com/send?text=%e5%88%86%e5%b8%83%e5%bc%8f%e7%bc%93%e5%ad%98%e4%b8%80%e8%87%b4%e6%80%a7%e9%97%ae%e9%a2%98%20-%20https%3a%2f%2fblog.hypertars.com%2fposts%2fdeveloper%2fdistributed_systems%2fcache%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 分布式缓存一致性问题 on telegram" href="https://telegram.me/share/url?text=%e5%88%86%e5%b8%83%e5%bc%8f%e7%bc%93%e5%ad%98%e4%b8%80%e8%87%b4%e6%80%a7%e9%97%ae%e9%a2%98&url=https%3a%2f%2fblog.hypertars.com%2fposts%2fdeveloper%2fdistributed_systems%2fcache%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>