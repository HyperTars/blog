[{"content":"1. Introduction 论文：\n https://raft.github.io/raft.pdf https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf  实现\n Golang: https://github.com/hashicorp/raft ETCD: https://github.com/etcd-io/etcd TiKV: https://github.com/tikv/tikv TiDB: https://github.com/pingcap/tidb RocksDB: https://github.com/facebook/rocksdb GoRaft: https://github.com/goraft/raft Animation http://www.kailing.pub/raft/index.html http://thesecretlivesofdata.com/raft/  共识算法三个主要特性  保证在任何非拜占庭情况下的正确性。可以解决网络延迟、网络分区、丢包、重复发送、乱序问题，无法解决拜占庭问题（存储不可靠、消息错误、分区、冗余、丢失、乱序）。 保证在大多数机器正常的情况下集群的高可用性，而少部分机器缓慢不影响整个集群的性能。 不依赖外部时间来保证日志的一致性。共识算法不受硬件影响，不因外部因素造成错误。但也造成了一些限制，让共识算法受网络影响较大，在异地容灾的情况下，共识算法的支持性比较差。  Raft 共识算法的区别特征  Strong Leader：在 raft 中，日志只能从 leader 流向其他服务器，这简化了复制日志的管理，使得 raft 更好理解。 Leader election：raft 使用随即计时器进行 leader 选举。这只需在任何共识算法都需要的心跳上增加少量机制，同时能够简单快速地解决冲突。 Membership changes：raft 使用一种共同一致 joint consensus 方法来处理集群成员变更的问题，变更时，两种不同的配置大多数机器会重叠，这允许整个集群在配置变更期间可以持续正常运行。   长声明周期的强 leader，是 raft 实现起来简单，区别于其他共识算法最重要的特点，同时也存在性能隐患。raft 选举出来的 leader 必须具有日志完整性。为了保证时时刻刻都能有具备完整日志的节点可以成为 leader，raft 必须使用顺序日志复制的方法来避免日志空洞。这一套是 raft 的三个子问题领导者选举、日志复制、安全性的三个闭环逻辑。为了支持强 leader，raft 单独分解出了领导者选举这个子问题。并用安全性子问题来保证选举出的 leader 具有完整日志，以及处理 leader 宕机的情况。强 leader 使得共识算法中最重要的日志复制模块实现起来很简单，同时也极大降低了 raft 实现和理解的难度。  2. Paper 2.1 复制状态机 Replicated State Machine  相同的初始状态 + 相同的输入 = 相同的结束状态 多个节点上，从相同的初始状态开始，执行相同的一串命令，产生相同的最终状态。 在 Raft 中，leader 将客户端请求 (command) 封装到一个个 log entry 中，将这些 log entries 复制到所有 follower 节点，然后大家按相同顺序应用 log entries 中的 command，根据复制状态机的理论，大家的结束状态肯定是一致的。 实现共识算法就是为了实现复制状态机。一个分布式场景下的各节点间，就是通过共识算法来保证命令序列的一致，从而始终保持他们的状态一致，从而实现高可用。 同时，复制状态机的功能可以更加强大，比如两个副本一个采用行存储（更适合 OLTP），一个采用列存储（更适合 OLAP），只要初始数据相同，并持续发给相同的命令，那么同一时刻从两个副本中读取到的结果也是一样的。这就是 HTAP（Hybrid Transaction and Analytical Process 混合事务和分析处理） 的实现方法（如 TiDB）。   共识算法的本质是实现复制状态机（目的即为容错，但是实现之后可以有更多用处）。  构建分布式存储系统，是为了获取更大的存储容量 Scalability 为了获取更大的存储容量，把数据进行分片 Sharding 更多的机器带来了更高的出错频率 Fault 为了容错 Fault Tolerance，对每个分片建立副本 Replication 为了维持副本间的一致，引入共识算法 Consensus 而共识算法会需要额外的资源与性能 Low Performance，这里又会反过来影响系统的容量和分片数设计     应用  数据量小：集群成员信息、配置文件、分布式锁、小容量分布式任务队列  无 leader 的共识算法（Basic Paxos）：Chubby、Zookeeper    数据量大：大规模存储系统  有 leader 的共识算法（Multi Paxos、Raft）：GFS、HDFS    数据量大，数据之间还存在关联：数据分片 partition 到多个状态机中，通过两阶段提交 2PC 保证一致性  Spanner、OceanBase、TiDB 等支持分布式事务的分布式数据库       2.2 状态简化 Raft Basics  任何时刻，一个服务器节点都处于 leader、follower、candidate 三个状态之一。 相较于 Paxos，极大简化了算法的实现，因为 Raft 只需要考虑状态的切换，而不用像 Paxos 考虑状态之前的共存和相互影响。   任何一个节点启动时都是 follower 状态 如果察觉到集群中没有 leader 的话，就会转换为 candidate 状态（第一个发现的先发优势大概率会成为 leader） 在 candidate 状态下经历一次或多次选举，如果选举结果是自己成为 leader，就会转换为 leader 状态，并为客户端提供服务；否则切换为 follower 状态 如果 leader 状态任期结束或者自身发生宕机等其他问题，就会转换为 follower 状态进入下一轮循环   Raft 把时间分割为任意长度的任期 (term)，任期用连续的整数标记（通常为 int）。 每一段任期从一次选举开始。在某些情况下，一次选举无法选出 leader（比如两个节点收到了相同的票数），在这种情况下，这一任期会以没有 leader 结束；一个新的任期（包含一次新的选举）会很快重新开始。Raft 保证在任意一个任期内，最多只有一个 leader。 任期的机制可以非常明确地标识集群的状态，并且通过任期的比较，可以确认一台服务器历史的状态，根据一台服务器是否有某个任期的日志确认其该期间是否宕机。   Raft 算法中服务器节点使用 RPC 进行通信，并且 Raft 中只有两种主要的 RPC  RequestVote RPC（请求投票）：由 candidate 在选举期间发起 AppendEntries RPC（追加条目）：由 leader 发起，用来复制日志和提供心跳机制   服务器之间通信的时候会交换当前任期号，如果一个服务器上的当前任期号比其他的小，该服务器会将自己的任期号更新为较大的那个值。 如果一个 candidate 或者 leader 发现自己的任期号过期了，他会立即回到 follower 状态 如果一个节点接收到一个包含过期的任期号的请求，他会直接拒绝这个请求。  2.3 领导者选举 Leader Election   Raft 内部有一种心跳机制，如果存在 leader，那么会周期性地向所有 follower 发送心跳，来维持自己的地位。如果 follower 一段时间没有收到心跳，他会认为系统中没有可用的 leader，然后开始进行选举。\n  开始一个选举过程后，follower 先增加自己的当前任期号，并转换到 candidate 状态，然后投票给自己，并且并行地向集群中的所有其他服务器节点发送投票请求（RequestVote RPC）\n  选举最终会有三种结果\n 它获得超过半数选票赢得选举：成为 leader 并开始发送心跳 其他节点赢得了选举：收到新 leader 的心跳后，如果新 leader 的任期号不小于（大于等于）自己当前的任期号，那么就从 candidate 回到 follower 状态。 一段时间后没有任何获胜者：每个 candidate 都在一个自己的随机选举超时时间后增加任期号开始新一轮投票。    为什么会没有获胜者？比如有多个 follower 同时称为 candidate，得票过于分散，没有任何一个 candidate 得票超过半数。\n  当前选举阶段没有产生任何 leader 并不需要集群中所有节点对此产生共识，而是通过每个 candidate 都在等待一个随机选举超时后，默认进入下一个选举阶段。\n  随机选举超时时间？150 - 300ms \u0026lt;img src=\u0026ldquo;http://thesecretlivesofdata.com/raft)\n  对于没有成为 candidate 的 follower 节点，对于同一任期，会按照先来先得的原则投出自己的选票。\n  RequestVote RPC 中要有 candidate 最后一个日志的信息：安全性子问题会给出进一步说明。\n  请求和返回都带有 term 任期号，Raft 需要通过任期号确定自身状态并判断接不接受该 RPC。\n  Follower 投票逻辑\n term 是否比自己大 lastLogIndex、lastLogTerm 安全性检查。 每个 follower 只有一张选票，先来先得。    请求投票 RPC Request (Golang) Arguments by candidates\n1 2 3 4 5 6  type RequestVoteRequest struct {  Term int // 自己当前的任期号  CandidateID int // 自己的 ID  LastLogIndex int // 自己的最后一个日志号（安全性）  LastLogTerm int // 自己最后的一个日志任期（安全性） }     请求投票 RPC Response (Golang) Results by followers\n1 2 3 4  type RequestVoteResponse struct {  Term int // 自己当前任期号  VoteGranted bool // 自己会不会投票给这个 candidate }     2.4 日志复制 Log Replication   Leader 被选取出来后开始为客户端提供服务。客户端随机向一个节点发送请求（或老 leader），如果这个节点正好是 leader，执行指令即可。若该节点是 follower，其根据心跳告知客户端该找谁。如果该节点正好宕机，客户端会找另一个节点重复上述操作。Raft 保证集群超过半数节点可用即可提供正常服务。\n  Leader 接收到客户端的指令后，会把指令作为一个新的条目追加到日志中。\n  一条日志中需要有三个信息\n 状态机指令 leader 的任期号：检测多个日志副本之间的不一致情况和判定节点状态；每个方块上的数字即任期号，一个任期用一个颜色标记。 日志号（日志索引）：区分日志前后关系，在图最上方，单调递增。leader 宕机会造成日志号相同的日志内容却不同，所以只有日志号和任期号才能唯一确定一个日志。    生成日志后，leader 会并行发送 AppendEntries RPC 给所有 follower，这样 follower 可以按照 leader 的日志顺序接收，复制该条目。当该条目被超过半数 follower 复制后，leader 就可以在本地执行该指令并把结果返回客户端。\n  本地执行命令，也就是 leader 应用日志到状态机这一步，称作提交。\n  若 leader 在提交之前宕机，虽然日志复制到了超过半数的节点，但没能提交。   Raft 一致性检查：leader 在每一个发往 follower 的追加条目 RPC 中，会放入前一个日志条目的索引位置和任期号，如果 follower 在它的日志中找不到前一个日志，那么它就会拒绝此日志，leader 收到 follower 的拒绝后，会发送前一个日志条目，从而逐渐向前定位到 follower 第一个缺失的日志。\n 优化不必要：失败不太可能发生，也不太可能会有很多不一致的日志条目。 可能的优化：follower 包含冲突条目的任期号和自己存储的那个任期第一个 index，leader 可以跳过那个任期内所有冲突的日志条目来减小 nextIndex，这样每个有冲突日志的条目的任期需要一个 AppendEntries RPC 而不是每个条目一次。    leader 或 follower 随时都有崩溃或缓慢的可能性，Raft 必须要在有宕机的情况下继续支持日志复制，并且保证每个副本日志顺序一致，以保证复制状态机的实现。follower 追不上 leader 有三种情况。\n follower 缓慢：如果有 leader 因为某些原因没有给 leader 响应，那么 leader 会不断重发追加条目请求（AppendEntries RPC），哪怕 leader 已经回复了客户端（超过半数节点回复，已提交）。 follower 宕机：如果有 follower 崩溃后恢复（可能期间已经换了几个 term 甚至几个不同 leader），这时 Raft 追加条目的一致性检查生效，保证 follower 能按顺序恢复崩溃后的缺失日志。 leader 宕机：leader 上可能有未提交的日志，而投票选举阶段不考虑这些日志。这意味着新选出的 leader 可能不具备这些未提交的日志。这里对外是可以接受的，客户端会认为那些未提交的日志是直接失败的，不会影响一致性。但是 leader 宕机后恢复因为这些未提交的日志会和新 leader 的日志产生冲突。老 leader 在恢复后成为 follower，进行一致性检查的过程中会发现自己最后的几个日志和新 leader 的日志不同，一些复制了未提交日志的 follower 也可能遇到这种情况。  如图，最上面为当前 leader，此时 follower 中的 c 和 d 比这个 leader 还多出两个日志，但是这些多出的日志并未提交，所以不构成多数。在这个集群中，leader 可以依靠 a b e f 和自己的选票当选 leader。 此时，raft 通过强制 follower 复制 leader 的日志来解决不一致的问题，因为这部分达成共识的才是真正已提交的。leader 通过一致性检查，找到最后一个和自己一致的 follower 之后，就会把这之后和自己冲突的所有日志全部覆盖掉，因为抛弃未提交的日志是不违反一致性的。所以 c d e f 中和 leader 不一致的日志都会被覆盖掉。 而如果此时当前 leader 宕机，那么 a c d 是有机会成为 leader 的。若 c 和 d 成为 leader，就会将当前自己多出来的日志复制给 follower，再提交。       通过日志复制机制，leader 在当权之后不需要任何特殊的操作来使日志恢复到一致状态。其只需要按规则一直发送 AppendEntries RPC，follower 在回复 AppendEntries RPC 进行一致性检查时就可以自动趋于一致。\n  leader 从来不会覆盖或删除自己的日志条目，这样的复制机制可以保证一致性。（Append-Only）\n 只要过半的服务器能正常运行，Raft 就能够接受、复制并应用新的日志条目 在正常情况下，新的日志条目可以在一个 RPC 来回中被复制给集群中的过半机器 单个运行慢的 follower 不会影响整体的性能。    追加日志 RPC Request (Golang)\n1 2 3 4 5 6 7 8  type AppendEntriesRequest struct {  Term int // 当前自己的任期号  LeaderID int // leader（也就是自己）的ID  PrevLogIndex int // 前一个日志的日志号，一致性检查  PrevLogTerm int // 前一个日志的任期号，一致性检查  Entries []LogEntry // 当前日志体  LeaderCommit int // leader 的已提交日志号（安全性） }     只有 leader 确认过半节点成功接收后才会提交，此时 follower 才可以真正提交。如果 LeaderCommit \u0026gt; CommitIndex，那么把 CommitIndex 设为 min(LeaderCommit, Index of Last New Entry)，即这一段的所有日志都是可以提交的。即 follower 会比 leader 晚一个日志的提交。\n  追加日志 RPC Response (Golang)\n1 2 3 4  type AppendEntriesResponse struct {  Term int // 当前自己的任期号  Success bool // 如果 follower 包括前一个日志成功 }     Success 必须 RequestTerm \u0026gt;= 自己的 Term，且通过一致性检查\n  2.5 安全性 Safety   领导者选举和日志复制两个子问题实际上已经涵盖了共识算法的全程，但这两点还不能完全保证每一个状态机会按照相同的顺序执行相同的命令。这里日志应用到状态机的顺序是一定不能颠倒的，很多共识算法为了效率允许日志乱序复制到非 leader 的节点，这样会导致日志中出现很多空洞，造成非常多的边界情况需要处理。Raft 为了简化设计，避免了这些边界情况的复杂处理，在日志复制阶段就保证了日志的有序性且无空洞。   日志复制阶段对于顺序的保障是 leader 是正常工作的，如果 leader 出现宕机，末尾日志的状态就有可能不正常。。此时新 leader 是否具备这些不正常日志以及如何处理这些不正常日志十分关键，这也是 Raft 为数不多的需要处理的边界情况。安全性即 Raft 通过定义几个补充规则完善整个算法，可以在各类宕机问题下都不出错。\n    Leader 宕机处理：选举限制\n 如果一个 follower 落后了 leader 若干条日志（但没有漏一整个任期），那么下次选举中，按照领导者选举里的规则（任期号+1），依旧有可能当选 leader。它当选 leader 后就永远也无法可能补上之前缺失的日志，从而造成状态机之间的不一致。 所以，需要对领导者选举增加一个限制，保证选出来的 leader 一定包含了之前各任期的所有被提交的日志条目。 Raft 通过 RequestVote RPC 中最后两个参数 LastLogIndex 和 LastLogTerm 来实现这个限制。如果投票者自己的日志比 candidate 还新，那么他会拒绝掉这个投票请求。 新即比较两个日志中最后一条日志条目的索引值和任期号来定义谁的日志比较新。 如果两个日志最后条目的任期号不同，则任期号大的日志更新。 如果两个日志最后条目的任期号相同，日志较长（日志号更大的）日志更新。 下图 a 中 S1 是 leader；b 中 S1 崩溃后 S5 通过 S3 S4 选票赢得选举；c 中 S5 崩溃，S1 重启并选举成功，此时日志 2 已经被复制到了大多数机器上，但还没有被提交；d 中 S1 再次崩溃，S5 通过 S2 S3 S4 的选票能够再次选举成功。  为什么 S2 S3 会投票给 S5？因为其日志相同，但是 S5 的任期号更大。 日志 2 被复制到了大多数机器上但还未提交，由问题 2 来解决。       Leader 宕机处理：新 leader 是否提交之前任期内的日志条目\n 一旦当前任期内的某个日志条目已经存储到过半的服务器节点上，leader 就知道该日志可以被提交了。此时 leader 应用日志到自己的状态机上并返回给客户端，但是 follower 并没有应用到自己的状态机上，没有提交，所以对整个集群来说提交这个状态并没有构成大多数。 follower 如何知道自己可以提交？follower 的提交触发：下一个 AppendEntries RPC 中的 LeaderCommit 参数，通过该参数 follower 可以知道 leader 提交到了哪个日志，从而自己也可以引用这个日志。所以 follower 的提交在下一个日志或者心跳（心跳也是一种特殊的 AppendEntries RPC，和普通的相比缺少日志体）。 而在 leader 提交给客户端到通知 follower 提交之前（一个心跳时间内），如果 leader 宕机那么会出现返回给客户端成功，但是 follower 不会提交的情况。 单点提交 vs 集群提交？对于这个问题而言，raft 是一种底层的共识算法，和客户端的交互不应该是 raft 应该担心的，要避免这个问题，应用可以设置一个集群提交的概念，只有集群中超过半数的节点都完成提交，才认为集群提交完成，leader 可以通过 follower 返回的 success 与否判定这个是否已完成提交，所以 leader 可以很容易判断一个日志是否符合集群提交的条件（类似分布式事务 2PC）。然而实际上 leader 单点提交后就返回客户端，已经是安全的了，没有等待集群提交的必要。 对于分布式数据库而言，分布式提交阶段的宕机是很难处理的，很多时候 leader 一宕机，与 client 的连接就断了，很容易造成 commit 状态未知，后续 client 很难确认提交的最终状态。Ref：Google Percolator 事务模型。 如果某个 leader 在提交某个日志条目之前崩溃了，以后的 leader 会试图完成该日志条目复制（而非提交）。通过选举规则可以知道，一般情况下新 leader 一定有老 leader 已提交的日志，但这些老日志可能在新 leader 中还没有提交，这时新 leader 会尝试将这个日志复制给其他所有 follower。 如果某个 leader 在提交某个日志条目之前崩溃，以后的 leader 会试图完成该日志条目的复制（而非提交，不能通过心跳提交老日志）。  c 到 d 的情况，哪怕 S1 当选 leader，把日志 2 复制到了大多数节点，最终却被日志 3 覆盖了，也就是没有在集群中提交日志 2。如果 S1 在 c 时提交了日志 2，就会出现不一致，因为日志 2 的任期号是老的 2，假设 c 中 S1 重新当选 leader，在 S1 S2 S3 中都把日志 2 提交了，这时候集群中的大多数节点都提交了，若此时 S1 宕机，集群重新选举，S5 依靠最高的任期号 3，依旧可以拿到 S2 S3 S3 的选票，从而覆盖 日志 2，进入 d 的情况。   Raft 永远不会通过计算副本数目的方式来提交之前任期内的日志条目。只有自己任期内的日志才能通过计算副本数目来提交，因为可以确认自己当前的任期号是最大的。即新 leader 提交是危险的，但是复制是安全的，依旧会把老日志复制到 follower 节点。 在新 leader 在它的任期内产生一个新日志，在这个日志提交时，就可以提交这些老日志（如图中 c 到 e）。必须要是新任期内的日志提交，因为此时新 leader 才能把自己的 LeaderCommit 设置为新任期内日志的日志号。相当于用新 leader 新任期内的日志保护了老任期内的日志，这样老任期内的日志就不会再覆盖了。     Leader 宕机 Animation:  Raft Scope  .bilibili_shortcodes { position: relative; width: 100%; height: 0; padding-bottom: 66%; margin: auto; overflow: hidden; text-align: center; } .bilibili_shortcodes iframe { position: absolute; width: 100%; height: 100%; left: 0; top: 0; }               Follower 和 Candidate 宕机处理\n Follower 或 candidate 崩溃了，那么后续发送给他们的 RequestVote 和 AppendEntries RPC 都会失败。 Raft 用无限的重试在处理这种失败。如果崩溃的机器重启了，那么这些 RPC 就会成功地完成。 如果一个服务器在完成了 RPC，但是还没有响应的时候崩溃了，那么它重启之后就会再次收到同样的请求，Raft 的 RPC 都是幂等的。    时间与可用性限制\n Raft 算法整体不依赖客观时间，哪怕因为网络或其他因素，造成后发的 RPC 先到，也不会影响 Raft 的正确性（这点和 Google Spanner 不同） 只要整个系统满足 广播时间 Broadcast Time \u0026laquo; 选举超时时间 Election Timeout \u0026laquo; 平均故障时间 MTBF，Raft 就可以选举出并维持一个稳定的 leader 广播时间和平均故障时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft 的 RPC 需要接受并将信息落盘，所以广播时间大概是 0.5ms - 20ms，取决于存储技术。因此，选举超时时间可能需要设置在 10ms - 500ms 之间。大多数服务器的平均故障时间间隔都在几个月甚至更长。     Raft 实现常用补丁 no-operation  一个节点当选 leader 后，立刻发送一个自己当前任期的的空日志体的 AppendEntries RPC。这样就可以把之前任期内满足提交条件的日志都提交了。 一旦 no-op 完成复制，就可以把之前任期内符合提交条件的日志保护起来了，从而就可以使他们安全提交。因为没有日志体，这个过程应该是很快的。     2.6 集群成员变更 Cluster Membership Changes   在需要改变集群配置的时候（增减节点、替换宕机机器、改变复制程度），Raft 可以进行配置变更自动化。\n  难点：保证转换过程中不会出现同一任期的两个 leader，因为转换期间整个集群可能划分为两个独立的大多数。（脑裂问题）\n  下图为三节点（S1 S2 S3）集群扩容到五节点（S1 S2 S3 S4 S5）\n S1 S2 为老配置集群，S3 S4 S5 为新配置集群 老配置为三节点，S1 S2 可以选出一个 leader （2票/3总） 新配置为五节点，S3 S4 S5 可以选出一个 leader （3票/5总）     两阶段配置变更\n 集群先切换到一个过渡配置，称为联合一致（joint consensus） 第一阶段，leader 发起 C(old, new)，使整个集群进入联合一致状态。此时，所有 RPC 都要在新旧配置中都达到大多数才算成功。（AppendEntries RPC）避免脑裂的核心问题点。 第二阶段，leader 发起 C(new)，使整个集群进入新配置状态。这是，所有 RPC 只要在新配置下能达到大多数就算成功。    一旦某个服务器将该新配置日志条目增加到自己的日志中，他就会用该配置来做出未来所有的决策（服务器总是使用它日志中的最新配置，无论该配置日志是否已被提交）。\n 这意味着 leader 不用等待 C(old, new) 和 C(new) 返回，只要发出去之后，就会直接使用其中的新规则来做出决策。    假设 leader 可以在集群成员变更任何时候宕机，有如下几种可能。\n leader 在 C(old, new) 未提交时宕机：raft 还未进入联合一致状态，这时 leader 宕机，可以仅靠老配置选出新 leader。 leader 在 C(old, new) 已提交但 C(new) 未发起时宕机：raft 集群进入联合一致状态，这时 leader 宕机，选出的新 leader 也要符合联合一致的选票规则。 leader 在 c(new) 已发起时宕机：集群可以仅靠新配置进行选举和日志复制。 缩减集群的情况下，leader 可能自身就是缩减的对象，那么它会在 C(new) 复制完成后自动退位。     增加 S4 S5 后，raft 会先将其设置为只读，等其追上日志进度后，才会开始集群成员变更。\n  现任 leader S3 发起 C(old, new)，并复制给了 S4 S5，此时有可能出现脑裂，S3 S4 S5 已经进入了联合一致状态，他们的决策要在新旧两个配置中都达到大多数才算成功。\n    leader 在 C(old, new) 未提交时宕机：此时 S1 S2 都是老配置，开始进行选举，并且可以以 （2/3） 产生一个老配置的 leader，但是在联合一致状态下，S3 S4 S5 中的任意节点必须要在老配置 S1 S2 S3 和新配置 S1-S5 下都拿到超过半数的选票才能当选，因为 S1 S2 投票给了他们之中的一个节点，所以在老配置中超过半数的条件是不满足的，导致 S3 S4 S5 中无法再选出一个 leader，这样集群变更就失败了，但是可以避免出现两个 leader 的脑裂情况。这里还有一种可能，重新选出的新 leader 具有 C(old, new)，如下图 S1 S3 S4 S5 都复制了C(old, new)，但还没有提交，从而选出来的新 leader 可以具有C(old, new)，但是按照安全性的限制，这新 leader 无法提交C(old, new)，不过可以让其继续发送 C(new)，继续进行集群成员变更。   leader 在 C(old, new) 已提交但 C(new) 未发起时宕机：假设 S3 没有宕机，正常复制 C(old, new) 满足联合一致条件，如图中 S2 S3 S4 都复制了C(old, new)，这时老配置中 S2 S3 超过了半数，新配置中 S2 S3 S4 也超过了半数，这时 C(old, new) 就可以提交了，若 C(new) 在未发起时发生了宕机，选举限制安全性规则决定了选出的新 leader 一定是具有 C(old, new) 的，也就是符合在两种配置集群中都超过半数的情况，所以不存在出现脑裂问题。此外，集群变更状态过程中，在联合一致的状态下也是可以正常执行命令，对外提供服务的，但是需要在两个配置集群中都达到大多数，才可以提交日志。C(old, new) 提交后，leader 就会发起 C(new)，这时 leader 只要满足新配置中的条件，就可以提交日志。   leader 在 c(new) 已发起时宕机：S3 S4 S5 都复制了 C(new) 日志，C(new) 就可以提交了，不用再在 S1 S2 S3 中达到大多数了，这时若 S3 发生宕机，已复制了 C(new) 的节点，会只按照新配置进行选举，没有复制 C(new) 的节点，会按照新老配置选举。不论是否复制到 C(new) 节点都有可能当上 leader，但没有复制到 C(new) 的节点，选举成功也会发送 C(new)，这里不会有问题。   leader 在 c(new) 已提交后宕机：但是有一种缩减节点的情况如 S1 - S5 缩减为 S1 S2 S3，C(old, new) 仍需要复制到两个集群中的大多数才能提交，但 C(new) 只需要复制到 S1 S2 S3 中的两个就可以提交了。这时如果 S3 宕机，已提交的 C(new) 并不会被覆盖。因为处于联合一致状态的节点，也就是只复制了 C(old, new)，没有复制 C(new) 的节点，必须要在两个集群中都达到大多数选票才能够选举成功。而 S2 S3 不会投票给 S1 S4 S5 中的任意一个。所以 S3 若发生宕机，只有 S2 才可以当选，已提交的 C(new) 并不会被覆盖。   C(old, new) 的复制满足了在新老配置中都超过半数的条件，但 leader 宕机，这时新 leader 无法提交 C(old, new)，但继续发 c(new) 的情况。如图中，leader S3 复制 C(old, new) 到了新老配置的大多数节点，满足联合一致，但 S3 未提交 C(old, new) 就宕机了。这时 S1 当选 leader，根据安全性规则，S1 不可以直接提交 C(old, new)，所以 S1 只能继续复制 c(new)，这时其把 c(new) 复制到了 S1 S4 S5 节点，构成了新配置集群的大多数，但这时其并不能提交，因为没有 S3 的反馈， C(old, new) 的提交规则并没有满足，这样提交的 c(new) 会把 C(old, new) 一并提交，是不安全的。论文中没有给出这种情况的解决方法，但是某些实现中，可以强制让 c(new) 按照联合一致的规则提交。如果当前 leader 在一段时间后还满足不了这个提交条件，那么其就会自动退位。\n    集群成员变更补充规则\n 新增节点时，需要等新增的节点完成日志同步再开始集群成员变更。防止集群在新增节点还未同步日志时就进入联合一致状态或新配置状态，影响正常命令日志提交。 缩减节点时，leader 本身可能就是要缩减的节点，这时它会在完成 c(new) 的提交后自动退位。在发起 c(new) 后，要退出集群的 leader 就会处在操纵一个不包含它本身的 raft 集群的状态下，这时它可以发送 c(new) 日志，但是日志计数时不计自身。 为了避免下线的节点超时选举而影响集群运行，服务器会在它确信集群中有 leader 时拒绝 RequestVote RPC。因为 c(new) 的新 leader 不会再发送心跳给要退出的节点，如果这些节点没有及时下线，他们会超时增加任期号后发送 RequestVote RPC。虽然他们不可能当选 leader，但会导致 raft 集群进入投票选举阶段，影响集群的正常运行。为了解决这个问题，Raft 在 RequestVote RPC 上补充了一条规则：一个节点如果在最小超时时间之内收到了 RequestVote RPC，那么它会拒绝此 RPC。这样，只要 follower 连续收到 leader 的心跳，那么退出集群节点的 RequestVote RPC 就不会影响到 raft 集群的正常运行了。    这种集群成员变更方法被称为 joint consensus 方法，或多节点变更方法。\n  由于上述方法边界情况较多较为复杂，实际实现大多基于另一种单节点变更方法，可以极大简化变更难度。即：一次只增减一个节点，新旧配置集群的大多数是一定会有重合的。这样，就可以不经过联合一致，直接从老配置切换到新配置。如果要变更多个节点，只需要多次执行单节点变更即可。\n cons：联合一致支持机器替换，abc -\u0026gt; bcd 仅需一次变更，单节点方法需要 abc -\u0026gt; abcd -\u0026gt; bcd cons：选择集群的高可用节点数时一般选择奇数来达到多数派的最高性价比，单节点变更过程必然会出现偶数个节点的情况。  TiDB 的解决方法：优化偶数节点集群的大多数概念。因为只需要让新老配置集群有交集就可以了，所以老配置的任意两个节点，如 ab ac bc 也可以作为变更过程中四节点的大多数，来让 c(new) 提交。因为 ab ac bc 是新老配置的最小交集，只要他们都复制了 c(new)，就可以保证选出的新 leader 是应用了最新配置的，不会发生脑裂问题。 TiDB 在 Raft 成员变更上踩的坑 后分布式时代：多数派读写的少数派实现   cons：连续两次变更，第一步变更如果出现了切主，那么紧跟着的下一次变更可能出现错误。  解决办法：新 leader 必须提交一条自己任期内的 no-op 日志，才能开始单节点集群成员变更。可以通过 no-op 把未提交的 c(new) 覆盖掉。 Raft 成员变更的工程实践      总结 Raft 五条公理\n     特性 解释     选举安全特性 对于一个给定的任期号，最多只会有一个领导人被选举出来   领导人只附加原则 领导人绝对不会删除或者覆盖自己的日志，只会增加   日志匹配原则 如果两个日志在相同的索引位置的日志条目的任期号相同，那么久认为这个日志从头到这个索引位置之间全部相同   领导人完全特性 如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中   状态机安全特性 如果一个领导人已经将给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会应用一个不同的日志    2.7 日志压缩 Log Compaction   需要一个机制清理 raft 上的 log，否则随着事件积累会打爆状态机的内存\n  如何判断日志可清理？使用快照技术，类似关系型数据库。每个节点达到一定条件后，可以把当前日志中的命令都写入自己的快照，然后就可以把已经并入快照的日志都删除了。\n  快照中一个 key 只会留有最新的一份 value，占用空间比内存小得多。\n  如果一个 follower 落后 leader 很多，如果老的日志被清理了，leader 怎么同步给 follower 呢？ raft 的策略是直接向 follower 发送自己的快照。\n  如下图，把日志 5 及之前的日志都并入快照。这部分日志将被直接清理。   因为大型分布式系统的状态机一般都很大，使用快照同步会比单一状态同步慢，因此何时进行快照、内存占用率等等也成为一个重要调参方向。\n  2.8 读操作处理 Read Only Operation  直观上讲，raft 的读只要直接读取 leader 上的结果就行了 直接从 leader 的状态机取值，实际上并不是线性一致性读（一般也称作强一致性读） 线性一致性读：读到的结果是读请求发起时已经完成提交的结果（快照） 在 leader 和其他节点发生了网络分区的情况下，其他节点可能已经重新选出了一个 leader，如果老 leader 在没有访问其他节点的情况下直接拿自身的值返回给客户端，这个读取的结果就有可能不是最新的。 所以，要追求强一致性读的话，就需要让这个读的过程或结果，也在大多数节点上达到共识。 稳妥的方法：把读也作为一个 log，由 leader 发到所有的节点上寻求共识，这个读的 log 提交后，得到的结果一定符合线性一致性。 优化后的方法要符合以下规则：  线性一致性读一定要发往 leader 如果一个 leader 在它的任期内还没有提交一个日志，那么它要在提交了一个日志后才能反馈 client 的读请求（可以通过 no-op 补丁来优化）。因为只有在自己任期内提交了一个日志，leader 才能确认之前任期的哪些日志已经被提交，才不会出现已提交的数据读取不到的情况。 在进行读操作前，leader 要向所有节点发送心跳，并得到大多数节点的反馈，确保自己仍是 leader leader 把自己已提交的日志号设为 readIndex，只要 leader 应用到了 readIndex 的日志，就可以查询状态机结果并返回 client 了。   优化后的线性一致性读也至少需要一轮 RPC（leader 确认心跳），并不比写操作快多少（最少也就一轮 RPC）。因为这轮读 RPC 仅仅是为了确认集群中没有新 leader 产生，那么如果 leader 上一次心跳发送的时间还不到选举超时时间下界，集群就不能选出一个新 leader，那么这段时间就可以不经过这轮心跳确认，直接返回读的结果。（不建议，因为时钟偏移、GC 等情况，通常认为时间不可靠） 如果不要求强一致性读，可以利用 follower 承载更大的读压力，类似数据库  follower 接受到读请求后，向 leader 请求 readIndex follower 等待自身状态机应用日志到 readIndex follower 查询状态机结果，并返回客户端   快速响应：最快的半数+1个节点响应即可 每个客户端应该维持一个 latestIdx 值，每个节点在接受读请求的时候与自己的 lastApplied 比较，如果这个值大于自己的 lastApplied，客户端重定向到一个 lastApplied 大于等于自己 latestIdx 的请求，并且每次读取请求都会返回这个节点的 lastApplied 值，客户端将 latestIdx 更新为此值，保持读取的线性一致性。  2.9 分析性能 Raft Evaluation   最基本的：每完成一个日志（命令）的复制与提交，需要的网络（RPC）来回次数。raft 在理想情况下，只需要一次 AppendEntries RPC 来回即可提交日志（理论上的极限）\n  影响 Raft 性能的因素及优化方法\n 生成快照：日志无限增长打满磁盘造成可用性问题  节点状态保存为 LSM Tree，存储最后应用日志的索引与任期，保证日志匹配特性 限定日志文件大小到达某一阈值后立即生成快照 使用写时复制，状态机的函数式顺序性天然支持   调节参数  心跳的随机时间，过快增加网络负载，过慢导致感知领导者崩溃的时间更长 选举的随机时间，如果大部分跟随者同时变为候选人则会导致选票被瓜分   流批结合  使用 Batch 能明显提升性能，如对于 RocksDB 的写，通常不会每次写入一个值，而是用 WriteBatch 缓存一批修改后整个写入，对于 Raft 来说，leader 可以一次收集多个 requests，一批发送给 follower（一个日志包含多个命令，然后批量复制，节省网络） 使用 Pipeline 让 leader 不用等待 follower 回复，继续给 follower 下一个日志。leader 维护一个 NextIndex 变量表示下一个给 follower 发送的 log 位置，只要 leader 和 follower 建立了连接，就可以认为网络稳定互通。所以当 leader 给 follower 发送了一批 log 后，可以直接更新 NextIndex，并立刻发送后面的 log，不需要等 follower 返回。如果网络出现了错误或者 follower 返回错误，就需要重新调整 NextIndex，重新发送 log。   并行追加  append log 涉及到落盘，有开销，完全可以在 leader 落盘的同时让 follower 尽快收到 log 并 append。如果一个 log 被大多数节点 append，就可以认为这个 log 被 committed 了，即时 leader 自己 append log 失败 panic 了，只要大多数 follower 能接受并成功 append，仍然可以认为这个 log 被 committed 了，后续就一定能 apply。 虽然 leader 能在 append log 之前给 follower 发 log，但是 follower 不能在 append log 之前告诉 leader 已经成功 append。。如果 follower 提前告知但后序实际失败，leader 仍然会认为这个 log 已经成功 committed，这样就有丢失数据的风险。   异步应用 asynchronous apply  被 committed 的 log 在什么时候被 apply 都不会影响数据的一致性，所以可以当一个 log 被 committed 之后，用另一个线程去异步 apply 这个 log。那么整个 raft 流程变为：  leader 接受一个 client 发送的 request leader 将对应的 log 发送给其他 follower 并本地 append leader 继续接受其他 client 的 request，持续执行上一步 leader 发现 log 已经被 committed，在另一个线程 apply leader 异步 apply log 之后，返回结果给对应的 client   以上，完全并行处理 append log 和 apply log，对于多个 clients 来说，整体的并发量和吞吐上去了。   Multi-Raft：将数据分组，每组数据独立，用自己的 raft 来同步。    Raft 优化\n   Raft 与 Paxos 比较  Paxos 实际上指一个能完美处理所有日志空洞带来的边界情况，并能保证处理这些边界情况的代价，要小于允许日志空洞带来的收益的共识算法。 raft 确实有不允许日志空洞这个性能上限，但大部分系统实现，连 raft 的上限都远远没有达到，所以无需考虑 raft 本身的瓶颈 raft 允许日志空洞的改造：Parallel Raft    ","permalink":"https://blog.hypertars.com/posts/developer/distributed_systems/raft/","summary":"1. Introduction 论文： https://raft.github.io/raft.pdf https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf 实现 Golang: https://github.com/hashicorp/raft ETCD: https://github.com/etcd-io/etcd TiKV: https://github.com/tikv/tikv TiDB: https://github.com/pingcap/tidb RocksDB: https://github.com/facebook/rocksdb GoRaft: https://github.com/goraft/raft Animation http://www.kailing.pub/raft/index.html http://thesecretlivesofdata.com/raft/ 共识算法三个主要特性 保证在任何非拜占庭情况下的正确性。可以解决网络延迟、网络分区、丢包、重","title":"Raft 分布式共识算法"},{"content":"","permalink":"https://blog.hypertars.com/posts/developer/distributed_systems/zab/","summary":"","title":"ZAB 分布式原子广播协议"},{"content":"1. 事务与分布式事务 1.1 事务 事务是数据库管理系统执行过程中的一个逻辑单元，能够保证一个事务中所有操作要么全部执行，要么全不执行。\n数据库事务拥有四个特性 ACID，分别为 Atomicity 原子性、Consistency 一致性、Isolation 隔离性、Durability 持久性。\n1.2 分布式事务 分布式事务指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点上。一个流程操作不取决于本地的数据库操作。分布式事务就是为了保证不同数据库的一致性。\n分布式事务之所以复杂，一个主要原因是同一个事务之间的执行多段代码会因为网络的不稳定造成失败等问题。当通过网络请求其他服务的接口时，可能得到 正确、失败、超时 三种结果。无论成功或失败都能得到唯一确定的结果，但是超时却不能确定接收者是否成功处理了请求，而这也成为造成诸多问题的诱因。系统之间的通信可靠性从单一系统中的可靠变成了微服务结构之间的不可靠，分布式事务其实就是在不可靠的通信下实现事务的特性。\n1.3 分布式事务基础：一致性   强一致性\n 任何一次都能读到某个数据的最近一次写数据。系统中的所有进程，看到的操作顺序，都和全局时钟下的顺序一致。任意时刻，所有节点中的数据都是一样的。    弱一致性\n 数据更新后，如果能容忍后序的访问只能访问到部分或者全部访问不到，则是弱一致性。    最终一致性\n 不保证在任意时刻任意节点上的同一份数据都相同的，但是随着时间的推移，不同节点上的同一份数据总是在向趋同的方向变化。一段时间后，节点间的数据会最终达到一致状态。    1.4 分布式事务基础：CAP 原则 CAP 原则又称为 CAP 定理，指在一个分布式系统中，Consistency 一致性、Availability 可用性、Partition-tolerance 分区容错性 三者不可兼得。\n Consistency 一致性：在分布式系统中的所有数据备份，在同一时刻是否同样的值（等同于所有节点访问同一份最新的数据副本） Availability 可用性：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求（对数据更新具备高可用性） Partition-tolerance 分区容错性：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。  分布式事务会部分遵循 ACID 规范\n 原子性：严格遵循 一致性：事务完成后的一致性严格遵循；事务中的一致性可适当放宽 隔离性：并行事务间不可影响；事务中间结果可见性允许安全放宽 持久性：严格遵循  因为事务过程中，不是一致的，但事务会最终完成，最终达到一致，所以我们把分布式事务称为“最终一致”\n1.5 分布式事务基础：BASE 理论 BASE aka Basically Availability Soft State Eventual Consistency\n Basically Availability 基本可用：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用 Soft State 软状态：允许系统存在中间状态，而该中间状态不会影响系统整体可用性 Eventual Consistency 最终一致性：系统中的所有数据副本经过一定时间后，最终能够达到一致的状态  BASE 理论本质上是对 CAP 理论的延伸，是对 AP 方案的一个补充\n1.6 CP + HA 分布式事务是在分布式环境下，遵循CAP理论，在CAP三者中只能够三选二，那么分布式事务是那种的组合呢？分布式事务是CP+HA，其中A是没有完全符合，但是能够达到Highly-Available，即高可用。\n近些年分布式理论进一步发展，产生了Paxos、Raft等CP的协议，在这基础上，加上硬件稳定性升级，可以在保证CP的情况下，做到高可用。谷歌分布式锁Chubby的公开数据显示，集群能提供99.99958％的平均可用性，一年也就130s的运行中断，已经能够满足非常严苛的应用要求。现在的SQL类数据库软件，都是走CP+HA，只是HA会比谷歌的这个极致数据更低一些，但一般都能够达到4个9\nCP+HA意味着不是BASE，意味着你只要写入成功，那么接下来的读，能够读取到最新的结果，开发人员不用担心读取到的不是最新数据，在多副本读写上面，与单机是一致的。\n2. 分布式事务解决方案 2.1 XA (2PC) XA 规范是 X/Open 组织定义的分布式事务处理 Distributed Transaction Processing 标准。XA 事务由一个或多个资源管理器（RM）、一个事务管理器（TM）和一个应用程序（ApplicationProgram）组成。\nXA 规范描述了全局的事务管理器与局部的资源管理器之间的接口。XA 规范的目的是允许的多个资源（数据库、应用服务器、消息队列等）在同一事务中访问，这样可以使 ACID 属性跨越应用程序而保持有效。XA 规范主要定义了(全局)事务管理器(TM)和(局部)资源管理器(RM)之间的接口。本地的数据库如 mysql 在 XA 中扮演的是 RM 角色。\nXA 规范使用两阶段提交（2PC Two-Phase Commit）来保证所有资源同时提交或回滚任何特定的事务。\nXA 流程\n 阶段一 Prepare：事务管理器向所有本地资源管理器发送请求，询问是否处于 ready 状态，所有参与者都将本事务是否成功的信息反馈发给协调者。即所有的参与者 RM 准备执行事务并锁住需要的资源。参与者 ready 时，向 TM 报告已准备就绪。 阶段二 Commit / Rollback：当事务管理者(TM)确认所有参与者(RM)都ready后，向所有参与者发送commit命令。事务管理器根据所有本地资源管理器的反馈，通知所有本地资源管理器，步调一致地在所有分支上提交或者回滚。  目前主流的数据库基本都支持XA事务，包括mysql、oracle、sqlserver、postgre。\n优点\n 对业务侵⼊很小，它最⼤的优势就是对使⽤⽅透明，用户可以像使⽤本地事务⼀样使⽤基于 XA 协议的分布式事务，能够严格保障事务 ACID 特性。  缺点\n 同步阻塞：当参与事务者存在占用公共资源的情况，其中一个占用了资源，其他事务参与者就只能阻塞等待资源释放，处于阻塞状态。对资源进行了长时间的锁定，并发度低。 单点故障：一旦事务管理器出现故障，整个系统不可用 数据不一致：在阶段二，如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。 不确定性：当协事务管理器发送 commit 之后，并且此时只有一个参与者收到了 commit，那么当该参与者与事务管理器同时宕机之后，重新选举的事务管理器无法确定该条消息是否提交成功。 XA要求数据库本身提供对规范和协议的支持  2.2 AT AT 对业务代码完全无侵入性，使用非常简单，改造成本低。我们只需要关注自己的业务SQL，AT模式下框架会通过分析我们业务SQL，反向生成回滚数据。\nAT 包含两个阶段\n 一阶段，所有参与事务的分支，本地事务Commit 业务数据和回滚日志（undoLog） 二阶段，事务协调者根据所有分支的情况，决定本次全局事务是Commit 还是 Rollback（二阶段是完全异步）  核心流程\n 为每个事务参与者的数据库创建一个 undo log 表，用于记录 DML SQL 执行前的行记录 每个事务参与者实际执行 DML SQL 时，生成修改前数据插入到 undo log 表中  流程图\n拓扑图\n优点\n 该事务模式使用方式，类似XA模式，业务无需编写各类补偿操作，回滚由框架自动完成  缺点\n 也类似XA，存在较长时间的锁，不满足高并发的场景。从性能的角度看，AT模式会比XA更高一些，但也带来了脏回滚这样的新问题。  2.3 TCC TCC 完整语义 Try-Confirm-Cancel\n Try：尝试执行，完成所有业务检查（一致性），预留必须业务资源（准隔离性），如加锁，加标记字段。TCC事务机制中的业务逻辑（Try），从执行阶段来看，与传统事务机制中业务逻辑相同。但从业务角度来看，却不一样。TCC机制中的Try仅是一个初步操作，它和后续的确认一起才能真正构成一个完整的业务逻辑。TCC机制将传统事务机制中的业务逻辑一分为二，拆分后保留的部分即为初步操作（Try）；而分离出的部分即为确认操作（Confirm），被延迟到事务提交阶段执行。TCC事务机制以初步操作（Try）为中心的，确认操作（Confirm）和取消操作（Cancel）都是围绕初步操作（Try）而展开。因此，Try阶段中的操作，其保障性是最好的，即使失败，仍然有取消操作（Cancel）可以将其不良影响进行回撤。可以认为  1  [传统事务机制]的业务逻辑 = [TCC事务机制]的初步操作（Try） + [TCC事务机制]的确认逻辑（Confirm）。    Confirm：真正确认执行业务，不做任务业务检查，只用 Try 阶段预留的业务资源；操作必须满足幂等性，失败后需要进行重试。确认操作（Confirm）是对初步操作（Try）的一个补充。当TCC事务管理器决定commit全局事务时，就会逐个执行初步操作（Try）指定的确认操作（Confirm），将初步操作（Try）未完成的事项最终完成。 Cancel：取消执行，释放 Try 阶段预留的业务资源；操作必须满足幂等性，异常处理和 Confirm 一致。取消操作（Cancel）是对初步操作（Try）的一个回撤。当TCC事务管理器决定rollback全局事务时，就会逐个执行初步操作（Try）指定的取消操作（Cancel），将初步操作（Try）已完成的事项全部撤回。  在 Try 阶段，对业务系统进行检查及资源预览，比如订单和存储操作，需要检查库存剩余数量是否够用，并进行预留，预留操作的话就是新建一个可用库存数量字段，Try 阶段操作是对这个可用库存数量进行操作。\n基于 TCC 实现分布式事务，会将原来只需要一个接口就可以实现的逻辑拆分为 Try、Confirm、Cancel 三个接口，所以代码实现复杂度相对较高，对业务的侵入性大。\nTCC的Confirm/Cancel阶段在业务逻辑上是不允许返回失败的，如果因为网络或者其他临时故障，导致不能返回成功，TM会不断的重试，直到Confirm/Cancel返回成功。\nTCC特点如下\n 并发度较高，无长期资源锁定。 开发量较大，需要提供Try/Confirm/Cancel接口。 一致性较好，不会发生SAGA已扣款最后又转账失败的情况 TCC适用于订单类业务，对中间状态有约束的业务  TCC 和 2PC？ TCC 并不是 2PC 的一种。2PC 需要 RM 提供底层支持（一般是兼容 XA），而 TCC 不需要。对于传统事务机制 X/Open XA 2PC，其特征在于它不依赖资源管理器 Resource Manager 对 XA 的支持，而是通过对（由业务系统提供的）业务逻辑的调度来实现分布式事务。\n对于业务系统中一个特定的业务逻辑S，其对外提供服务时，必须接受一些不确定性，即对业务逻辑执行的一次调用仅是一个临时性操作，调用它的消费方服务M保留了后续的取消权。如果M认为全局事务应该rollback，它会要求取消之前的临时性操作，这将对应S的一个取消操作；而当M认为全局事务应该commit时，它会放弃之前临时性操作的取消权，这对应S的一个确认操作。\n每一个初步操作，最终都会被确认或取消。因此，针对一个具体的业务服务，TCC事务机制需要业务系统提供三段业务逻辑：初步操作Try、确认操作Confirm、取消操作Cancel。\nTCC 在事务处理上，要么调用 confirm，要么调用 cancel；try 逻辑与全局事务无关。 使用 2PC 时完整的事务生命周期是：begin -\u0026gt; 业务逻辑 -\u0026gt; prepare -\u0026gt; commit 使用 TCC 时完整的事务生命周期是：begin -\u0026gt; 业务逻辑 (try) -\u0026gt; commit (confirm 业务)\n在执行阶段可以将二者一一对应\n 2PC 业务阶段 - TCC try 业务阶段 2PC 提交阶段 (prepare \u0026amp; commit) - TCC confirm 提交阶段 2PC 回滚阶段 (rollback) - TCC cancel 回滚阶段  TCC 不是 2PC，只是它对事务的 提交/回滚 是通过执行一段 confirm/cancel 业务逻辑来实现而已。\nTCC 事务管理器协调者设计 TCC全局事务必须基于 RM 本地事务来实现全局事务\n Try/Confirm/Cancel业务在执行时，会访问资源管理器（Resource Manager，下文简称RM）来存取数据。这些存取操作，必须要参与RM本地事务，以使其更改的数据要么都commit，要么都rollback。 分布式事务管理框架的职责，不是做出全局事务提交/回滚的指令，而是管理全局事务提交/回滚的过程。  实际实现中：\n 只实现 try cancel 就是一个 SAGA  一般扣除类动作，try 时即扣除，cancel 时补偿，confirm 为空；因为 confirm 时再完成扣除可能已经余额不足，因此要求 confirm 一定成功。 但是发放类动作，一般不适用 SAGA，因为 cancel 前可能已经被消耗，导致无法回滚，此时必须用 TCC 语义在 confirm 时是肌肤放。   只实现 try confirm 就是一个 XA  TCC 语义与问题解决 子事务接口约束\n 中间态设计：try 成功后，confirm / cancel 必须成功 try / confirm / cancel 幂等 cancel 允许空回滚 cancel 允许请求倒挂（try 悬挂）  问题：Try 超时（空回滚）\n Cancel 接口设计时允许空回滚。在 try 接口因为丢包没有收到时，事务管理器会触发回滚，这时会出发 Cancel 接口，若发现没有对应事务的 xid 或主键时，需要返回回滚成功。让事务服务管理器认为已回滚，否则会不断重试。  问题：Confirm / Cancel 超时\n 因为网络抖动或拥堵可能超时，事务管理器会对资源进行重试操作，为了不因为重复调用而多次占用资源，设计时进行幂等控制。  问题：Try 比 Cancel 晚到达（try 悬挂）\n Cancel 比 try 先执行，事务管理器生成回滚，而之后收到了 try 接口调用。此时 try 接口因 cancel 已执行而不再执行，否则产生数据不一致。所以在 try 前先检查该事务 xid 或业务主键，如果已经标记为回滚成功过，就不再执行 try 的业务操作。  2.4 SAGA SAGA事务，其核心思想是将长事务拆分为多个本地短事务，由Saga事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。每个参与者都定义一个正向行为（变更）和一个反向行为（补偿），分布式事务按照既定顺序执行正向行为直到全部成功（提交），如果中间发生错误，则逆序执行对应的反向行为（回滚）。\n适用场景\n 业务流程长、业务流程多 长事务适用，对中间结果不敏感的业务场景适用 参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口  优势\n 并发度高，不用像XA事务那样长期锁定资源 一阶段提交本地事务，无锁，高性能 事件驱动架构，参与者可异步执行，高吞吐 需要定义正常操作以及补偿操作，开发量比XA大，但是补偿服务易于实现  缺点\n 不保证隔离性 一致性较弱，对于转账，可能发生A用户已扣款，最后转账又失败的情况  2.5 本地消息表 本地消息表方案中会有消息生产者与消费者两个角色，假设服务 A 是消息的生产者，服务 B 是消息的消费者。写本地消息和业务操作放在一个事务里，保证了业务和发消息的原子性，要么他们全都成功，要么全都失败。此方案的核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。人工重试更多的是应用于支付场景，通过对账系统对事后问题的处理。\n 当服务 A 被其他服务调用发生数据库表更操作，首先会更新该服务数据库的业务表，其次会往相同数据库的消息表中插入一条数据，两个操作发生在同一个事务中 服务 A 的脚本定期轮询本地消息往 mq 中写入一条消息，如果消息发送失败会进行重试 服务 B 消费 mq 中的消息，并处理业务逻辑。如果本地事务处理失败，会在继续消费 mq 中的消息进行重试，如果业务上的失败，可以通知服务 A 进行回滚操作，或进行告警通知相关人员进行处理。  本地消息表的实现条件\n 消费者与生成者的接口都要幂等 生产者需要额外的创建消息记录表 可能需要提供补偿逻辑，如果消费者业务失败，生产者可支持回滚操作 进行重试或进行业务逻辑告警，通过人工介入的方式处理。  适用场景\n 可异步执行的业务，且后续操作无需回滚的业务  容错机制\n 扣减余额事务 失败时，事务直接回滚，无后续步骤 轮序生产消息失败， 增加余额事务失败都会进行重试  本地消息表的特点\n 不支持回滚 轮询生产消息难实现，如果定时轮询会延长事务总时长，如果订阅binlog则开发维护困难 适用于可异步执行的业务，且后续操作无需回滚的业务  2.6 事务消息 在上述的本地消息表方案中，生产者需要额外创建消息表，还需要对本地消息表进行轮询，业务负担较重。阿里开源的RocketMQ 4.3之后的版本正式支持事务消息，该事务消息本质上是把本地消息表放到RocketMQ上，解决生产端的消息发送与本地事务执行的原子性问题。\n事务消息发送及提交\n 发送消息（half消息） 服务端存储消息，并响应消息的写入结果 根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行） 根据本地事务状态执行Commit或者Rollback（Commit操作发布消息，消息对消费者可见）  补偿流程\n 对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次“回查” Producer收到回查消息，返回消息对应的本地事务的状态，为Commit或者Rollback 事务消息方案与本地消息表机制非常类似，区别主要在于原先相关的本地表操作替换成了一个反查接口  事务消息特点\n 长事务仅需要分拆成多个任务，并提供一个反查接口，使用简单 事务消息的回查没有好的方案，极端情况可能出现数据错误 适用于可异步执行的业务，且后续操作无需回滚的业务  适用场景\n 可异步执行的业务，且后续操作无需回滚的业务  2.7 最大努力通知 发起通知方通过一定的机制最大努力将业务处理结果通知到接收方。具体包括：\n 有一定的消息重复通知机制。因为接收通知方可能没有接收到通知，此时要有一定的机制对消息重复通知。 消息校对机制。如果尽最大努力也没有通知到接收方，或者接收方消费消息后要再次消费，此时可由接收方主动向通知方查询消息信息来满足需求。  前面的本地消息表和事务消息都属于可靠消息，与这里的最大努力通知有什么不同？\n 可靠消息一致性，发起通知方需要保证将消息发出去，并且将消息发到接收通知方，消息的可靠性关键由发起通知方来保证。 最大努力通知，发起通知方尽最大的努力将业务处理结果通知为接收通知方，但是可能消息接收不到，此时需要接收通知方主动调用发起通知方的接口查询业务处理结果，通知的可靠性关键在接收通知方。  解决方案上，最大努力通知需要：\n 提供接口，让接受通知放能够通过接口查询业务处理结果 消息队列ACK机制，消息队列按照间隔1min、5min、10min、30min、1h、2h、5h、10h的方式，逐步拉大通知间隔 ，直到达到通知要求的时间窗口上限。之后不再通知  适用类型\n 业务通知类型，例如微信交易的结果，就是通过最大努力通知方式通知各个商户，既有回调通知，也有交易查询接口  ","permalink":"https://blog.hypertars.com/posts/developer/distributed_systems/transaction/","summary":"1. 事务与分布式事务 1.1 事务 事务是数据库管理系统执行过程中的一个逻辑单元，能够保证一个事务中所有操作要么全部执行，要么全不执行。 数据库事务拥有四","title":"分布式事务"}]