[{"content":"1. Introduction 论文：\n https://raft.github.io/raft.pdf https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf  实现\n Golang: https://github.com/hashicorp/raft ETCD: https://github.com/etcd-io/etcd TiKV: https://github.com/tikv/tikv TiDB: https://github.com/pingcap/tidb RocksDB: https://github.com/facebook/rocksdb GoRaft: https://github.com/goraft/raft Animation http://www.kailing.pub/raft/index.html http://thesecretlivesofdata.com/raft/  共识算法三个主要特性  保证在任何非拜占庭情况下的正确性。可以解决网络延迟、网络分区、丢包、重复发送、乱序问题，无法解决拜占庭问题（存储不可靠、消息错误、分区、冗余、丢失、乱序）。 保证在大多数机器正常的情况下集群的高可用性，而少部分机器缓慢不影响整个集群的性能。 不依赖外部时间来保证日志的一致性。共识算法不受硬件影响，不因外部因素造成错误。但也造成了一些限制，让共识算法受网络影响较大，在异地容灾的情况下，共识算法的支持性比较差。  Raft 共识算法的区别特征  Strong Leader：在 raft 中，日志只能从 leader 流向其他服务器，这简化了复制日志的管理，使得 raft 更好理解。 Leader election：raft 使用随即计时器进行 leader 选举。这只需在任何共识算法都需要的心跳上增加少量机制，同时能够简单快速地解决冲突。 Membership changes：raft 使用一种共同一致 joint consensus 方法来处理集群成员变更的问题，变更时，两种不同的配置大多数机器会重叠，这允许整个集群在配置变更期间可以持续正常运行。   长声明周期的强 leader，是 raft 实现起来简单，区别于其他共识算法最重要的特点，同时也存在性能隐患。raft 选举出来的 leader 必须具有日志完整性。为了保证时时刻刻都能有具备完整日志的节点可以成为 leader，raft 必须使用顺序日志复制的方法来避免日志空洞。这一套是 raft 的三个子问题领导者选举、日志复制、安全性的三个闭环逻辑。为了支持强 leader，raft 单独分解出了领导者选举这个子问题。并用安全性子问题来保证选举出的 leader 具有完整日志，以及处理 leader 宕机的情况。强 leader 使得共识算法中最重要的日志复制模块实现起来很简单，同时也极大降低了 raft 实现和理解的难度。  2. Paper 2.1 复制状态机 Replicated State Machine  相同的初始状态 + 相同的输入 = 相同的结束状态 多个节点上，从相同的初始状态开始，执行相同的一串命令，产生相同的最终状态。 在 Raft 中，leader 将客户端请求 (command) 封装到一个个 log entry 中，将这些 log entries 复制到所有 follower 节点，然后大家按相同顺序应用 log entries 中的 command，根据复制状态机的理论，大家的结束状态肯定是一致的。 实现共识算法就是为了实现复制状态机。一个分布式场景下的各节点间，就是通过共识算法来保证命令序列的一致，从而始终保持他们的状态一致，从而实现高可用。 同时，复制状态机的功能可以更加强大，比如两个副本一个采用行存储（更适合 OLTP），一个采用列存储（更适合 OLAP），只要初始数据相同，并持续发给相同的命令，那么同一时刻从两个副本中读取到的结果也是一样的。这就是 HTAP（Hybrid Transaction and Analytical Process 混合事务和分析处理） 的实现方法（如 TiDB）。   共识算法的本质是实现复制状态机（目的即为容错，但是实现之后可以有更多用处）。  构建分布式存储系统，是为了获取更大的存储容量 Scalability 为了获取更大的存储容量，把数据进行分片 Sharding 更多的机器带来了更高的出错频率 Fault 为了容错 Fault Tolerance，对每个分片建立副本 Replication 为了维持副本间的一致，引入共识算法 Consensus 而共识算法会需要额外的资源与性能 Low Performance，这里又会反过来影响系统的容量和分片数设计     应用  数据量小：集群成员信息、配置文件、分布式锁、小容量分布式任务队列  无 leader 的共识算法（Basic Paxos）：Chubby、Zookeeper    数据量大：大规模存储系统  有 leader 的共识算法（Multi Paxos、Raft）：GFS、HDFS    数据量大，数据之间还存在关联：数据分片 partition 到多个状态机中，通过两阶段提交 2PC 保证一致性  Spanner、OceanBase、TiDB 等支持分布式事务的分布式数据库       2.2 状态简化 Raft Basics  任何时刻，一个服务器节点都处于 leader、follower、candidate 三个状态之一。 相较于 Paxos，极大简化了算法的实现，因为 Raft 只需要考虑状态的切换，而不用像 Paxos 考虑状态之前的共存和相互影响。   任何一个节点启动时都是 follower 状态 如果察觉到集群中没有 leader 的话，就会转换为 candidate 状态（第一个发现的先发优势大概率会成为 leader） 在 candidate 状态下经历一次或多次选举，如果选举结果是自己成为 leader，就会转换为 leader 状态，并为客户端提供服务；否则切换为 follower 状态 如果 leader 状态任期结束或者自身发生宕机等其他问题，就会转换为 follower 状态进入下一轮循环   Raft 把时间分割为任意长度的任期 (term)，任期用连续的整数标记（通常为 int）。 每一段任期从一次选举开始。在某些情况下，一次选举无法选出 leader（比如两个节点收到了相同的票数），在这种情况下，这一任期会以没有 leader 结束；一个新的任期（包含一次新的选举）会很快重新开始。Raft 保证在任意一个任期内，最多只有一个 leader。 任期的机制可以非常明确地标识集群的状态，并且通过任期的比较，可以确认一台服务器历史的状态，根据一台服务器是否有某个任期的日志确认其该期间是否宕机。   Raft 算法中服务器节点使用 RPC 进行通信，并且 Raft 中只有两种主要的 RPC  RequestVote RPC（请求投票）：由 candidate 在选举期间发起 AppendEntries RPC（追加条目）：由 leader 发起，用来复制日志和提供心跳机制   服务器之间通信的时候会交换当前任期号，如果一个服务器上的当前任期号比其他的小，该服务器会将自己的任期号更新为较大的那个值。 如果一个 candidate 或者 leader 发现自己的任期号过期了，他会立即回到 follower 状态 如果一个节点接收到一个包含过期的任期号的请求，他会直接拒绝这个请求。  2.3 领导者选举 Leader Election   Raft 内部有一种心跳机制，如果存在 leader，那么会周期性地向所有 follower 发送心跳，来维持自己的地位。如果 follower 一段时间没有收到心跳，他会认为系统中没有可用的 leader，然后开始进行选举。\n  开始一个选举过程后，follower 先增加自己的当前任期号，并转换到 candidate 状态，然后投票给自己，并且并行地向集群中的所有其他服务器节点发送投票请求（RequestVote RPC）\n  选举最终会有三种结果\n 它获得超过半数选票赢得选举：成为 leader 并开始发送心跳 其他节点赢得了选举：收到新 leader 的心跳后，如果新 leader 的任期号不小于（大于等于）自己当前的任期号，那么就从 candidate 回到 follower 状态。 一段时间后没有任何获胜者：每个 candidate 都在一个自己的随机选举超时时间后增加任期号开始新一轮投票。    为什么会没有获胜者？比如有多个 follower 同时称为 candidate，得票过于分散，没有任何一个 candidate 得票超过半数。\n  当前选举阶段没有产生任何 leader 并不需要集群中所有节点对此产生共识，而是通过每个 candidate 都在等待一个随机选举超时后，默认进入下一个选举阶段。\n  随机选举超时时间？150 - 300ms \u0026lt;img src=\u0026ldquo;http://thesecretlivesofdata.com/raft)\n  对于没有成为 candidate 的 follower 节点，对于同一任期，会按照先来先得的原则投出自己的选票。\n  RequestVote RPC 中要有 candidate 最后一个日志的信息：安全性子问题会给出进一步说明。\n  请求和返回都带有 term 任期号，Raft 需要通过任期号确定自身状态并判断接不接受该 RPC。\n  Follower 投票逻辑\n term 是否比自己大 lastLogIndex、lastLogTerm 安全性检查。 每个 follower 只有一张选票，先来先得。    请求投票 RPC Request (Golang) Arguments by candidates\n1 2 3 4 5 6  type RequestVoteRequest struct {  Term int // 自己当前的任期号  CandidateID int // 自己的 ID  LastLogIndex int // 自己的最后一个日志号（安全性）  LastLogTerm int // 自己最后的一个日志任期（安全性） }     请求投票 RPC Response (Golang) Results by followers\n1 2 3 4  type RequestVoteResponse struct {  Term int // 自己当前任期号  VoteGranted bool // 自己会不会投票给这个 candidate }     2.4 日志复制 Log Replication   Leader 被选取出来后开始为客户端提供服务。客户端随机向一个节点发送请求（或老 leader），如果这个节点正好是 leader，执行指令即可。若该节点是 follower，其根据心跳告知客户端该找谁。如果该节点正好宕机，客户端会找另一个节点重复上述操作。Raft 保证集群超过半数节点可用即可提供正常服务。\n  Leader 接收到客户端的指令后，会把指令作为一个新的条目追加到日志中。\n  一条日志中需要有三个信息\n 状态机指令 leader 的任期号：检测多个日志副本之间的不一致情况和判定节点状态；每个方块上的数字即任期号，一个任期用一个颜色标记。 日志号（日志索引）：区分日志前后关系，在图最上方，单调递增。leader 宕机会造成日志号相同的日志内容却不同，所以只有日志号和任期号才能唯一确定一个日志。    生成日志后，leader 会并行发送 AppendEntries RPC 给所有 follower，这样 follower 可以按照 leader 的日志顺序接收，复制该条目。当该条目被超过半数 follower 复制后，leader 就可以在本地执行该指令并把结果返回客户端。\n  本地执行命令，也就是 leader 应用日志到状态机这一步，称作提交。\n  若 leader 在提交之前宕机，虽然日志复制到了超过半数的节点，但没能提交。   Raft 一致性检查：leader 在每一个发往 follower 的追加条目 RPC 中，会放入前一个日志条目的索引位置和任期号，如果 follower 在它的日志中找不到前一个日志，那么它就会拒绝此日志，leader 收到 follower 的拒绝后，会发送前一个日志条目，从而逐渐向前定位到 follower 第一个缺失的日志。\n 优化不必要：失败不太可能发生，也不太可能会有很多不一致的日志条目。 可能的优化：follower 包含冲突条目的任期号和自己存储的那个任期第一个 index，leader 可以跳过那个任期内所有冲突的日志条目来减小 nextIndex，这样每个有冲突日志的条目的任期需要一个 AppendEntries RPC 而不是每个条目一次。    leader 或 follower 随时都有崩溃或缓慢的可能性，Raft 必须要在有宕机的情况下继续支持日志复制，并且保证每个副本日志顺序一致，以保证复制状态机的实现。follower 追不上 leader 有三种情况。\n follower 缓慢：如果有 leader 因为某些原因没有给 leader 响应，那么 leader 会不断重发追加条目请求（AppendEntries RPC），哪怕 leader 已经回复了客户端（超过半数节点回复，已提交）。 follower 宕机：如果有 follower 崩溃后恢复（可能期间已经换了几个 term 甚至几个不同 leader），这时 Raft 追加条目的一致性检查生效，保证 follower 能按顺序恢复崩溃后的缺失日志。 leader 宕机：leader 上可能有未提交的日志，而投票选举阶段不考虑这些日志。这意味着新选出的 leader 可能不具备这些未提交的日志。这里对外是可以接受的，客户端会认为那些未提交的日志是直接失败的，不会影响一致性。但是 leader 宕机后恢复因为这些未提交的日志会和新 leader 的日志产生冲突。老 leader 在恢复后成为 follower，进行一致性检查的过程中会发现自己最后的几个日志和新 leader 的日志不同，一些复制了未提交日志的 follower 也可能遇到这种情况。  如图，最上面为当前 leader，此时 follower 中的 c 和 d 比这个 leader 还多出两个日志，但是这些多出的日志并未提交，所以不构成多数。在这个集群中，leader 可以依靠 a b e f 和自己的选票当选 leader。 此时，raft 通过强制 follower 复制 leader 的日志来解决不一致的问题，因为这部分达成共识的才是真正已提交的。leader 通过一致性检查，找到最后一个和自己一致的 follower 之后，就会把这之后和自己冲突的所有日志全部覆盖掉，因为抛弃未提交的日志是不违反一致性的。所以 c d e f 中和 leader 不一致的日志都会被覆盖掉。 而如果此时当前 leader 宕机，那么 a c d 是有机会成为 leader 的。若 c 和 d 成为 leader，就会将当前自己多出来的日志复制给 follower，再提交。       通过日志复制机制，leader 在当权之后不需要任何特殊的操作来使日志恢复到一致状态。其只需要按规则一直发送 AppendEntries RPC，follower 在回复 AppendEntries RPC 进行一致性检查时就可以自动趋于一致。\n  leader 从来不会覆盖或删除自己的日志条目，这样的复制机制可以保证一致性。（Append-Only）\n 只要过半的服务器能正常运行，Raft 就能够接受、复制并应用新的日志条目 在正常情况下，新的日志条目可以在一个 RPC 来回中被复制给集群中的过半机器 单个运行慢的 follower 不会影响整体的性能。    追加日志 RPC Request (Golang)\n1 2 3 4 5 6 7 8  type AppendEntriesRequest struct {  Term int // 当前自己的任期号  LeaderID int // leader（也就是自己）的ID  PrevLogIndex int // 前一个日志的日志号，一致性检查  PrevLogTerm int // 前一个日志的任期号，一致性检查  Entries []LogEntry // 当前日志体  LeaderCommit int // leader 的已提交日志号（安全性） }     只有 leader 确认过半节点成功接收后才会提交，此时 follower 才可以真正提交。如果 LeaderCommit \u0026gt; CommitIndex，那么把 CommitIndex 设为 min(LeaderCommit, Index of Last New Entry)，即这一段的所有日志都是可以提交的。即 follower 会比 leader 晚一个日志的提交。\n  追加日志 RPC Response (Golang)\n1 2 3 4  type AppendEntriesResponse struct {  Term int // 当前自己的任期号  Success bool // 如果 follower 包括前一个日志成功 }     Success 必须 RequestTerm \u0026gt;= 自己的 Term，且通过一致性检查\n  2.5 安全性 Safety   领导者选举和日志复制两个子问题实际上已经涵盖了共识算法的全程，但这两点还不能完全保证每一个状态机会按照相同的顺序执行相同的命令。这里日志应用到状态机的顺序是一定不能颠倒的，很多共识算法为了效率允许日志乱序复制到非 leader 的节点，这样会导致日志中出现很多空洞，造成非常多的边界情况需要处理。Raft 为了简化设计，避免了这些边界情况的复杂处理，在日志复制阶段就保证了日志的有序性且无空洞。   日志复制阶段对于顺序的保障是 leader 是正常工作的，如果 leader 出现宕机，末尾日志的状态就有可能不正常。。此时新 leader 是否具备这些不正常日志以及如何处理这些不正常日志十分关键，这也是 Raft 为数不多的需要处理的边界情况。安全性即 Raft 通过定义几个补充规则完善整个算法，可以在各类宕机问题下都不出错。\n    Leader 宕机处理：选举限制\n 如果一个 follower 落后了 leader 若干条日志（但没有漏一整个任期），那么下次选举中，按照领导者选举里的规则（任期号+1），依旧有可能当选 leader。它当选 leader 后就永远也无法可能补上之前缺失的日志，从而造成状态机之间的不一致。 所以，需要对领导者选举增加一个限制，保证选出来的 leader 一定包含了之前各任期的所有被提交的日志条目。 Raft 通过 RequestVote RPC 中最后两个参数 LastLogIndex 和 LastLogTerm 来实现这个限制。如果投票者自己的日志比 candidate 还新，那么他会拒绝掉这个投票请求。 新即比较两个日志中最后一条日志条目的索引值和任期号来定义谁的日志比较新。 如果两个日志最后条目的任期号不同，则任期号大的日志更新。 如果两个日志最后条目的任期号相同，日志较长（日志号更大的）日志更新。 下图 a 中 S1 是 leader；b 中 S1 崩溃后 S5 通过 S3 S4 选票赢得选举；c 中 S5 崩溃，S1 重启并选举成功，此时日志 2 已经被复制到了大多数机器上，但还没有被提交；d 中 S1 再次崩溃，S5 通过 S2 S3 S4 的选票能够再次选举成功。  为什么 S2 S3 会投票给 S5？因为其日志相同，但是 S5 的任期号更大。 日志 2 被复制到了大多数机器上但还未提交，由问题 2 来解决。       Leader 宕机处理：新 leader 是否提交之前任期内的日志条目\n 一旦当前任期内的某个日志条目已经存储到过半的服务器节点上，leader 就知道该日志可以被提交了。此时 leader 应用日志到自己的状态机上并返回给客户端，但是 follower 并没有应用到自己的状态机上，没有提交，所以对整个集群来说提交这个状态并没有构成大多数。 follower 如何知道自己可以提交？follower 的提交触发：下一个 AppendEntries RPC 中的 LeaderCommit 参数，通过该参数 follower 可以知道 leader 提交到了哪个日志，从而自己也可以引用这个日志。所以 follower 的提交在下一个日志或者心跳（心跳也是一种特殊的 AppendEntries RPC，和普通的相比缺少日志体）。 而在 leader 提交给客户端到通知 follower 提交之前（一个心跳时间内），如果 leader 宕机那么会出现返回给客户端成功，但是 follower 不会提交的情况。 单点提交 vs 集群提交？对于这个问题而言，raft 是一种底层的共识算法，和客户端的交互不应该是 raft 应该担心的，要避免这个问题，应用可以设置一个集群提交的概念，只有集群中超过半数的节点都完成提交，才认为集群提交完成，leader 可以通过 follower 返回的 success 与否判定这个是否已完成提交，所以 leader 可以很容易判断一个日志是否符合集群提交的条件（类似分布式事务 2PC）。然而实际上 leader 单点提交后就返回客户端，已经是安全的了，没有等待集群提交的必要。 对于分布式数据库而言，分布式提交阶段的宕机是很难处理的，很多时候 leader 一宕机，与 client 的连接就断了，很容易造成 commit 状态未知，后续 client 很难确认提交的最终状态。Ref：Google Percolator 事务模型。 如果某个 leader 在提交某个日志条目之前崩溃了，以后的 leader 会试图完成该日志条目复制（而非提交）。通过选举规则可以知道，一般情况下新 leader 一定有老 leader 已提交的日志，但这些老日志可能在新 leader 中还没有提交，这时新 leader 会尝试将这个日志复制给其他所有 follower。 如果某个 leader 在提交某个日志条目之前崩溃，以后的 leader 会试图完成该日志条目的复制（而非提交，不能通过心跳提交老日志）。  c 到 d 的情况，哪怕 S1 当选 leader，把日志 2 复制到了大多数节点，最终却被日志 3 覆盖了，也就是没有在集群中提交日志 2。如果 S1 在 c 时提交了日志 2，就会出现不一致，因为日志 2 的任期号是老的 2，假设 c 中 S1 重新当选 leader，在 S1 S2 S3 中都把日志 2 提交了，这时候集群中的大多数节点都提交了，若此时 S1 宕机，集群重新选举，S5 依靠最高的任期号 3，依旧可以拿到 S2 S3 S3 的选票，从而覆盖 日志 2，进入 d 的情况。   Raft 永远不会通过计算副本数目的方式来提交之前任期内的日志条目。只有自己任期内的日志才能通过计算副本数目来提交，因为可以确认自己当前的任期号是最大的。即新 leader 提交是危险的，但是复制是安全的，依旧会把老日志复制到 follower 节点。 在新 leader 在它的任期内产生一个新日志，在这个日志提交时，就可以提交这些老日志（如图中 c 到 e）。必须要是新任期内的日志提交，因为此时新 leader 才能把自己的 LeaderCommit 设置为新任期内日志的日志号。相当于用新 leader 新任期内的日志保护了老任期内的日志，这样老任期内的日志就不会再覆盖了。     Leader 宕机 Animation:  Raft Scope  .bilibili_shortcodes { position: relative; width: 100%; height: 0; padding-bottom: 66%; margin: auto; overflow: hidden; text-align: center; } .bilibili_shortcodes iframe { position: absolute; width: 100%; height: 100%; left: 0; top: 0; }               Follower 和 Candidate 宕机处理\n Follower 或 candidate 崩溃了，那么后续发送给他们的 RequestVote 和 AppendEntries RPC 都会失败。 Raft 用无限的重试在处理这种失败。如果崩溃的机器重启了，那么这些 RPC 就会成功地完成。 如果一个服务器在完成了 RPC，但是还没有响应的时候崩溃了，那么它重启之后就会再次收到同样的请求，Raft 的 RPC 都是幂等的。    时间与可用性限制\n Raft 算法整体不依赖客观时间，哪怕因为网络或其他因素，造成后发的 RPC 先到，也不会影响 Raft 的正确性（这点和 Google Spanner 不同） 只要整个系统满足 广播时间 Broadcast Time \u0026laquo; 选举超时时间 Election Timeout \u0026laquo; 平均故障时间 MTBF，Raft 就可以选举出并维持一个稳定的 leader 广播时间和平均故障时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft 的 RPC 需要接受并将信息落盘，所以广播时间大概是 0.5ms - 20ms，取决于存储技术。因此，选举超时时间可能需要设置在 10ms - 500ms 之间。大多数服务器的平均故障时间间隔都在几个月甚至更长。     Raft 实现常用补丁 no-operation  一个节点当选 leader 后，立刻发送一个自己当前任期的的空日志体的 AppendEntries RPC。这样就可以把之前任期内满足提交条件的日志都提交了。 一旦 no-op 完成复制，就可以把之前任期内符合提交条件的日志保护起来了，从而就可以使他们安全提交。因为没有日志体，这个过程应该是很快的。     2.6 集群成员变更 Cluster Membership Changes   在需要改变集群配置的时候（增减节点、替换宕机机器、改变复制程度），Raft 可以进行配置变更自动化。\n  难点：保证转换过程中不会出现同一任期的两个 leader，因为转换期间整个集群可能划分为两个独立的大多数。（脑裂问题）\n  下图为三节点（S1 S2 S3）集群扩容到五节点（S1 S2 S3 S4 S5）\n S1 S2 为老配置集群，S3 S4 S5 为新配置集群 老配置为三节点，S1 S2 可以选出一个 leader （2票/3总） 新配置为五节点，S3 S4 S5 可以选出一个 leader （3票/5总）     两阶段配置变更\n 集群先切换到一个过渡配置，称为联合一致（joint consensus） 第一阶段，leader 发起 C(old, new)，使整个集群进入联合一致状态。此时，所有 RPC 都要在新旧配置中都达到大多数才算成功。（AppendEntries RPC）避免脑裂的核心问题点。 第二阶段，leader 发起 C(new)，使整个集群进入新配置状态。这是，所有 RPC 只要在新配置下能达到大多数就算成功。    一旦某个服务器将该新配置日志条目增加到自己的日志中，他就会用该配置来做出未来所有的决策（服务器总是使用它日志中的最新配置，无论该配置日志是否已被提交）。\n 这意味着 leader 不用等待 C(old, new) 和 C(new) 返回，只要发出去之后，就会直接使用其中的新规则来做出决策。    假设 leader 可以在集群成员变更任何时候宕机，有如下几种可能。\n leader 在 C(old, new) 未提交时宕机：raft 还未进入联合一致状态，这时 leader 宕机，可以仅靠老配置选出新 leader。 leader 在 C(old, new) 已提交但 C(new) 未发起时宕机：raft 集群进入联合一致状态，这时 leader 宕机，选出的新 leader 也要符合联合一致的选票规则。 leader 在 c(new) 已发起时宕机：集群可以仅靠新配置进行选举和日志复制。 缩减集群的情况下，leader 可能自身就是缩减的对象，那么它会在 C(new) 复制完成后自动退位。     增加 S4 S5 后，raft 会先将其设置为只读，等其追上日志进度后，才会开始集群成员变更。\n  现任 leader S3 发起 C(old, new)，并复制给了 S4 S5，此时有可能出现脑裂，S3 S4 S5 已经进入了联合一致状态，他们的决策要在新旧两个配置中都达到大多数才算成功。\n    leader 在 C(old, new) 未提交时宕机：此时 S1 S2 都是老配置，开始进行选举，并且可以以 （2/3） 产生一个老配置的 leader，但是在联合一致状态下，S3 S4 S5 中的任意节点必须要在老配置 S1 S2 S3 和新配置 S1-S5 下都拿到超过半数的选票才能当选，因为 S1 S2 投票给了他们之中的一个节点，所以在老配置中超过半数的条件是不满足的，导致 S3 S4 S5 中无法再选出一个 leader，这样集群变更就失败了，但是可以避免出现两个 leader 的脑裂情况。这里还有一种可能，重新选出的新 leader 具有 C(old, new)，如下图 S1 S3 S4 S5 都复制了C(old, new)，但还没有提交，从而选出来的新 leader 可以具有C(old, new)，但是按照安全性的限制，这新 leader 无法提交C(old, new)，不过可以让其继续发送 C(new)，继续进行集群成员变更。   leader 在 C(old, new) 已提交但 C(new) 未发起时宕机：假设 S3 没有宕机，正常复制 C(old, new) 满足联合一致条件，如图中 S2 S3 S4 都复制了C(old, new)，这时老配置中 S2 S3 超过了半数，新配置中 S2 S3 S4 也超过了半数，这时 C(old, new) 就可以提交了，若 C(new) 在未发起时发生了宕机，选举限制安全性规则决定了选出的新 leader 一定是具有 C(old, new) 的，也就是符合在两种配置集群中都超过半数的情况，所以不存在出现脑裂问题。此外，集群变更状态过程中，在联合一致的状态下也是可以正常执行命令，对外提供服务的，但是需要在两个配置集群中都达到大多数，才可以提交日志。C(old, new) 提交后，leader 就会发起 C(new)，这时 leader 只要满足新配置中的条件，就可以提交日志。   leader 在 c(new) 已发起时宕机：S3 S4 S5 都复制了 C(new) 日志，C(new) 就可以提交了，不用再在 S1 S2 S3 中达到大多数了，这时若 S3 发生宕机，已复制了 C(new) 的节点，会只按照新配置进行选举，没有复制 C(new) 的节点，会按照新老配置选举。不论是否复制到 C(new) 节点都有可能当上 leader，但没有复制到 C(new) 的节点，选举成功也会发送 C(new)，这里不会有问题。   leader 在 c(new) 已提交后宕机：但是有一种缩减节点的情况如 S1 - S5 缩减为 S1 S2 S3，C(old, new) 仍需要复制到两个集群中的大多数才能提交，但 C(new) 只需要复制到 S1 S2 S3 中的两个就可以提交了。这时如果 S3 宕机，已提交的 C(new) 并不会被覆盖。因为处于联合一致状态的节点，也就是只复制了 C(old, new)，没有复制 C(new) 的节点，必须要在两个集群中都达到大多数选票才能够选举成功。而 S2 S3 不会投票给 S1 S4 S5 中的任意一个。所以 S3 若发生宕机，只有 S2 才可以当选，已提交的 C(new) 并不会被覆盖。   C(old, new) 的复制满足了在新老配置中都超过半数的条件，但 leader 宕机，这时新 leader 无法提交 C(old, new)，但继续发 c(new) 的情况。如图中，leader S3 复制 C(old, new) 到了新老配置的大多数节点，满足联合一致，但 S3 未提交 C(old, new) 就宕机了。这时 S1 当选 leader，根据安全性规则，S1 不可以直接提交 C(old, new)，所以 S1 只能继续复制 c(new)，这时其把 c(new) 复制到了 S1 S4 S5 节点，构成了新配置集群的大多数，但这时其并不能提交，因为没有 S3 的反馈， C(old, new) 的提交规则并没有满足，这样提交的 c(new) 会把 C(old, new) 一并提交，是不安全的。论文中没有给出这种情况的解决方法，但是某些实现中，可以强制让 c(new) 按照联合一致的规则提交。如果当前 leader 在一段时间后还满足不了这个提交条件，那么其就会自动退位。\n    集群成员变更补充规则\n 新增节点时，需要等新增的节点完成日志同步再开始集群成员变更。防止集群在新增节点还未同步日志时就进入联合一致状态或新配置状态，影响正常命令日志提交。 缩减节点时，leader 本身可能就是要缩减的节点，这时它会在完成 c(new) 的提交后自动退位。在发起 c(new) 后，要退出集群的 leader 就会处在操纵一个不包含它本身的 raft 集群的状态下，这时它可以发送 c(new) 日志，但是日志计数时不计自身。 为了避免下线的节点超时选举而影响集群运行，服务器会在它确信集群中有 leader 时拒绝 RequestVote RPC。因为 c(new) 的新 leader 不会再发送心跳给要退出的节点，如果这些节点没有及时下线，他们会超时增加任期号后发送 RequestVote RPC。虽然他们不可能当选 leader，但会导致 raft 集群进入投票选举阶段，影响集群的正常运行。为了解决这个问题，Raft 在 RequestVote RPC 上补充了一条规则：一个节点如果在最小超时时间之内收到了 RequestVote RPC，那么它会拒绝此 RPC。这样，只要 follower 连续收到 leader 的心跳，那么退出集群节点的 RequestVote RPC 就不会影响到 raft 集群的正常运行了。    这种集群成员变更方法被称为 joint consensus 方法，或多节点变更方法。\n  由于上述方法边界情况较多较为复杂，实际实现大多基于另一种单节点变更方法，可以极大简化变更难度。即：一次只增减一个节点，新旧配置集群的大多数是一定会有重合的。这样，就可以不经过联合一致，直接从老配置切换到新配置。如果要变更多个节点，只需要多次执行单节点变更即可。\n cons：联合一致支持机器替换，abc -\u0026gt; bcd 仅需一次变更，单节点方法需要 abc -\u0026gt; abcd -\u0026gt; bcd cons：选择集群的高可用节点数时一般选择奇数来达到多数派的最高性价比，单节点变更过程必然会出现偶数个节点的情况。  TiDB 的解决方法：优化偶数节点集群的大多数概念。因为只需要让新老配置集群有交集就可以了，所以老配置的任意两个节点，如 ab ac bc 也可以作为变更过程中四节点的大多数，来让 c(new) 提交。因为 ab ac bc 是新老配置的最小交集，只要他们都复制了 c(new)，就可以保证选出的新 leader 是应用了最新配置的，不会发生脑裂问题。 TiDB 在 Raft 成员变更上踩的坑 后分布式时代：多数派读写的少数派实现   cons：连续两次变更，第一步变更如果出现了切主，那么紧跟着的下一次变更可能出现错误。  解决办法：新 leader 必须提交一条自己任期内的 no-op 日志，才能开始单节点集群成员变更。可以通过 no-op 把未提交的 c(new) 覆盖掉。 Raft 成员变更的工程实践      总结 Raft 五条公理\n     特性 解释     选举安全特性 对于一个给定的任期号，最多只会有一个领导人被选举出来   领导人只附加原则 领导人绝对不会删除或者覆盖自己的日志，只会增加   日志匹配原则 如果两个日志在相同的索引位置的日志条目的任期号相同，那么久认为这个日志从头到这个索引位置之间全部相同   领导人完全特性 如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中   状态机安全特性 如果一个领导人已经将给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会应用一个不同的日志    2.7 日志压缩 Log Compaction   需要一个机制清理 raft 上的 log，否则随着事件积累会打爆状态机的内存\n  如何判断日志可清理？使用快照技术，类似关系型数据库。每个节点达到一定条件后，可以把当前日志中的命令都写入自己的快照，然后就可以把已经并入快照的日志都删除了。\n  快照中一个 key 只会留有最新的一份 value，占用空间比内存小得多。\n  如果一个 follower 落后 leader 很多，如果老的日志被清理了，leader 怎么同步给 follower 呢？ raft 的策略是直接向 follower 发送自己的快照。\n  如下图，把日志 5 及之前的日志都并入快照。这部分日志将被直接清理。   因为大型分布式系统的状态机一般都很大，使用快照同步会比单一状态同步慢，因此何时进行快照、内存占用率等等也成为一个重要调参方向。\n  2.8 读操作处理 Read Only Operation  直观上讲，raft 的读只要直接读取 leader 上的结果就行了 直接从 leader 的状态机取值，实际上并不是线性一致性读（一般也称作强一致性读） 线性一致性读：读到的结果是读请求发起时已经完成提交的结果（快照） 在 leader 和其他节点发生了网络分区的情况下，其他节点可能已经重新选出了一个 leader，如果老 leader 在没有访问其他节点的情况下直接拿自身的值返回给客户端，这个读取的结果就有可能不是最新的。 所以，要追求强一致性读的话，就需要让这个读的过程或结果，也在大多数节点上达到共识。 稳妥的方法：把读也作为一个 log，由 leader 发到所有的节点上寻求共识，这个读的 log 提交后，得到的结果一定符合线性一致性。 优化后的方法要符合以下规则：  线性一致性读一定要发往 leader 如果一个 leader 在它的任期内还没有提交一个日志，那么它要在提交了一个日志后才能反馈 client 的读请求（可以通过 no-op 补丁来优化）。因为只有在自己任期内提交了一个日志，leader 才能确认之前任期的哪些日志已经被提交，才不会出现已提交的数据读取不到的情况。 在进行读操作前，leader 要向所有节点发送心跳，并得到大多数节点的反馈，确保自己仍是 leader leader 把自己已提交的日志号设为 readIndex，只要 leader 应用到了 readIndex 的日志，就可以查询状态机结果并返回 client 了。   优化后的线性一致性读也至少需要一轮 RPC（leader 确认心跳），并不比写操作快多少（最少也就一轮 RPC）。因为这轮读 RPC 仅仅是为了确认集群中没有新 leader 产生，那么如果 leader 上一次心跳发送的时间还不到选举超时时间下界，集群就不能选出一个新 leader，那么这段时间就可以不经过这轮心跳确认，直接返回读的结果。（不建议，因为时钟偏移、GC 等情况，通常认为时间不可靠） 如果不要求强一致性读，可以利用 follower 承载更大的读压力，类似数据库  follower 接受到读请求后，向 leader 请求 readIndex follower 等待自身状态机应用日志到 readIndex follower 查询状态机结果，并返回客户端   快速响应：最快的半数+1个节点响应即可 每个客户端应该维持一个 latestIdx 值，每个节点在接受读请求的时候与自己的 lastApplied 比较，如果这个值大于自己的 lastApplied，客户端重定向到一个 lastApplied 大于等于自己 latestIdx 的请求，并且每次读取请求都会返回这个节点的 lastApplied 值，客户端将 latestIdx 更新为此值，保持读取的线性一致性。  2.9 分析性能 Raft Evaluation   最基本的：每完成一个日志（命令）的复制与提交，需要的网络（RPC）来回次数。raft 在理想情况下，只需要一次 AppendEntries RPC 来回即可提交日志（理论上的极限）\n  影响 Raft 性能的因素及优化方法\n 生成快照：日志无限增长打满磁盘造成可用性问题  节点状态保存为 LSM Tree，存储最后应用日志的索引与任期，保证日志匹配特性 限定日志文件大小到达某一阈值后立即生成快照 使用写时复制，状态机的函数式顺序性天然支持   调节参数  心跳的随机时间，过快增加网络负载，过慢导致感知领导者崩溃的时间更长 选举的随机时间，如果大部分跟随者同时变为候选人则会导致选票被瓜分   流批结合  使用 Batch 能明显提升性能，如对于 RocksDB 的写，通常不会每次写入一个值，而是用 WriteBatch 缓存一批修改后整个写入，对于 Raft 来说，leader 可以一次收集多个 requests，一批发送给 follower（一个日志包含多个命令，然后批量复制，节省网络） 使用 Pipeline 让 leader 不用等待 follower 回复，继续给 follower 下一个日志。leader 维护一个 NextIndex 变量表示下一个给 follower 发送的 log 位置，只要 leader 和 follower 建立了连接，就可以认为网络稳定互通。所以当 leader 给 follower 发送了一批 log 后，可以直接更新 NextIndex，并立刻发送后面的 log，不需要等 follower 返回。如果网络出现了错误或者 follower 返回错误，就需要重新调整 NextIndex，重新发送 log。   并行追加  append log 涉及到落盘，有开销，完全可以在 leader 落盘的同时让 follower 尽快收到 log 并 append。如果一个 log 被大多数节点 append，就可以认为这个 log 被 committed 了，即时 leader 自己 append log 失败 panic 了，只要大多数 follower 能接受并成功 append，仍然可以认为这个 log 被 committed 了，后续就一定能 apply。 虽然 leader 能在 append log 之前给 follower 发 log，但是 follower 不能在 append log 之前告诉 leader 已经成功 append。。如果 follower 提前告知但后序实际失败，leader 仍然会认为这个 log 已经成功 committed，这样就有丢失数据的风险。   异步应用 asynchronous apply  被 committed 的 log 在什么时候被 apply 都不会影响数据的一致性，所以可以当一个 log 被 committed 之后，用另一个线程去异步 apply 这个 log。那么整个 raft 流程变为：  leader 接受一个 client 发送的 request leader 将对应的 log 发送给其他 follower 并本地 append leader 继续接受其他 client 的 request，持续执行上一步 leader 发现 log 已经被 committed，在另一个线程 apply leader 异步 apply log 之后，返回结果给对应的 client   以上，完全并行处理 append log 和 apply log，对于多个 clients 来说，整体的并发量和吞吐上去了。   Multi-Raft：将数据分组，每组数据独立，用自己的 raft 来同步。    Raft 优化\n   Raft 与 Paxos 比较  Paxos 实际上指一个能完美处理所有日志空洞带来的边界情况，并能保证处理这些边界情况的代价，要小于允许日志空洞带来的收益的共识算法。 raft 确实有不允许日志空洞这个性能上限，但大部分系统实现，连 raft 的上限都远远没有达到，所以无需考虑 raft 本身的瓶颈 raft 允许日志空洞的改造：Parallel Raft    ","permalink":"https://blog.hypertars.com/posts/developer/distributed_systems/raft/","summary":"1. Introduction 论文： https://raft.github.io/raft.pdf https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf 实现 Golang: https://github.com/hashicorp/raft ETCD: https://github.com/etcd-io/etcd TiKV: https://github.com/tikv/tikv TiDB: https://github.com/pingcap/tidb RocksDB: https://github.com/facebook/rocksdb GoRaft: https://github.com/goraft/raft Animation http://www.kailing.pub/raft/index.html http://thesecretlivesofdata.com/raft/ 共识算法三个主要特性 保证在任何非拜占庭情况下的正确性。可以解决网络延迟、网络分区、丢包、重","title":"Raft 分布式共识算法"},{"content":"","permalink":"https://blog.hypertars.com/posts/developer/distributed_systems/zab/","summary":"","title":"ZAB 分布式原子广播协议"}]